Linear Regression of an Indicator Matrix
----------------------------------------

è‹¥æˆ‘å€‘èªªï¼Œğ’¢ æœ‰ *K* å€‹é¡åˆ¥ï¼Œç‚ºäº†è¡¨ç¤ºè©²ç­†è³‡æ–™æ˜¯ä½•ç¨®é¡åˆ¥ï¼Œæˆ‘å€‘å¯ä»¥å»ºç«‹ä¸€å€‹
*Y* çŸ©é™£,
*Y*â€„=â€„(*Y*<sub>1</sub>,â€†...,â€†*Y*<sub>*k*</sub>)<sub>*N*â€…Ã—â€…*K*</sub>ï¼Œä¹Ÿå°±æ˜¯èªª*Y*çŸ©é™£ä¸­æœ‰*N*å€‹*K*ç¶­çš„
row vectorsã€‚è€Œæ ¹æ“šç·šæ€§è¿´æ­¸æ¨¡å‹ï¼Œæˆ‘å€‘å¯å¾—å° *Y*çš„ä¼°è¨ˆç‚ºï¼š

*YÌ‚*â€„=â€„*X*(*X*<sup>*T*</sup>*X*)<sup>â€…âˆ’â€…1</sup>*X*<sup>*T*</sup>*Y*â€„=â€„*X**BÌ‚*

*B*<sub>(*p*â€…+â€…1)â€…Ã—â€…*K*</sub>ï¼š*p* å€‹ inputs ç„¶å¾ŒåŠ ä¸Šæˆªè·é …ã€‚

æˆ–è€…æ˜¯å¾å¦å¤–ä¸€å€‹è§€é»ï¼Œä¹Ÿå°±æ˜¯æˆ‘å€‘å¸Œæœ›æ¥µå°åŒ–*yÌ‚*è·Ÿ *y*çš„è·é›¢ã€‚
$$\\min\_{\\bf{B}}\\sum\_{i=1}^N\|\|y\_i-\[(1,x\_i^T) \\textbf{B} \]^T \|\|^2$$
ä¹Ÿå°±æ˜¯èªªï¼Œ*fÌ‚*(*x*)æœƒåˆ†é¡åˆ°æœ€æ¥è¿‘çš„ç›®æ¨™ç¾¤(
*y*<sub>*i*</sub>â€„=â€„*t*<sub>*k*</sub>,Â *i**f*Â *g*<sub>*i*</sub>â€„=â€„*k*
)ï¼š

$$\\hat{G}(x)=\\mathop{\\arg\\min}\\limits\_{k}\|\|\\hat{f}(x)-t\_k\|\|^2$$
\#æ¨¡æ“¬Fig. 4.3Â¶
<a href="https://esl.hohoweiya.xyz/notes/LDA/sim-4-3/index.html" class="uri">https://esl.hohoweiya.xyz/notes/LDA/sim-4-3/index.html</a>

Linear Discriminant Analysis(LDA)
---------------------------------

åœ¨èª²æœ¬çš„2.4ç¯€ï¼Œæˆ‘å€‘çŸ¥é“åœ¨åšåˆ†é¡æ±ºç­–æ™‚ï¼Œæˆ‘å€‘æ˜¯åœ¨æ¥µå¤§åŒ–æŸç¨®å¾Œé©—æ©Ÿç‡(posrerior
probability)ï¼Œä¹Ÿå°±çµ¦å®š*X*â€„=â€„*x*æ™‚ï¼Œæ‰¾ä¸€å€‹æœ€å¤§å¯èƒ½æ€§çš„åˆ†é¡ç•¶ä½œåˆ†æçš„çµæœã€‚
ä¹Ÿå°±æ˜¯èªªï¼š æˆ‘å€‘ä»¤åˆ†é¡æ‰€åšçš„é æ¸¬ *GÌ‚*(*x*)â€„=â€„ğ’¢<sub>*k*</sub>
(ğ’¢<sub>*k*</sub>ç‚ºå…¶ä¸­ä¸€ç¨®åˆ†é¡)ï¼›
ä¹Ÿå°±è¡¨ç¤ºï¼Œç•¶çµ¦å®š*X*â€„=â€„*x*çš„æ¢ä»¶æ©Ÿç‡ä¹‹ä¸‹ï¼Œğ’¢<sub>*k*</sub>æ˜¯æœ€æœ‰å¯èƒ½çš„å‡ºè±¡ï¼Œæ›æˆæ•¸å­¸çš„èªè¨€å°±æ˜¯ï¼š
*P*(ğ’¢<sub>*k*</sub>\|*X*â€„=â€„*x*)â€„=â€„max<sub>*l*</sub>*P*(*G*â€„=â€„ğ’¢<sub>*l*</sub>\|*X*â€„=â€„*x*)
ï¼ˆæˆ–è€…èªªæ˜¯ â€„=â€„max<sub>*l*</sub>*P*(*G*â€„=â€„*l*\|*X*â€„=â€„*x*)ï¼‰

æœ‰é€™å€‹åŸºæœ¬è§€å¿µå¾Œï¼Œæˆ‘å€‘è¨­å®šè³‡æ–™å±¬æ–¼é¡åˆ¥*k*çš„å…ˆé©—æ©Ÿç‡ç‚ºï¼š*Ï€*<sub>*k*</sub>â€„=â€„*P*(*G*â€„=â€„*k*)ï¼Œè€Œç•¶ç„¶
$\\sum\_{k=1}^K\\pi\_k =1$ã€‚

è€Œé€éè²æ°å®šç†ï¼Œæˆ‘å€‘å¯ä»¥å¾—åˆ°ä»¥ä¸‹é—œä¿‚ï¼š

$$P(G=k\|X=x)=\\frac{{\\color{Red} f\_k(x)}{\\color{Blue} \\pi\_k}}{{\\color{DarkOrange} \\sum\_{l=1}^k f\_l(x)\\pi\_l}}=\\frac{{\\color{Red} P(X\|G=k)}{\\color{Blue} P(G=k)} }{{\\color{DarkOrange} \\sum\_l P(X\|G=l)P(G=l)}}$$

æ ¹æ“šä¸Šå¼ï¼Œæˆ‘å€‘å¿…é ˆå°*f*<sub>*k*</sub>(*x*)åšä¸€äº›å‡è¨­ï¼Œé€™ä¹Ÿå°±æ˜¯ç•¶è³‡æ–™æ˜¯ç¬¬*k*é¡æ™‚ï¼Œ*X*çš„æ©Ÿç‡å¯†åº¦å‡½æ•¸ã€‚æˆ‘å€‘è¨­å®šå„åˆ†é¡çš„æ©Ÿç‡å¯†åº¦æœå¾ã€Œ**å¤šè®Šé‡å¸¸æ…‹åˆ†é…(Multivariate
Normal Distribution)**ã€ã€‚

$$f\_k(x)=\\frac{1}{(2\\pi)^{\\frac{p}{2}}\|\\Sigma\_k\|^\\frac{1}{2}}e^{-\\frac{1}{2}(x-\\mu\_k)^T\\Sigma\_k^{-1}(x-\\mu\_k)}$$
è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨LDAçš„æ¶æ§‹ä¹‹ä¸‹ï¼Œæ‰€æœ‰é¡åˆ¥çš„pdfå‡äº«æœ‰ç›¸åŒçš„å…±è®Šæ•¸çŸ©é™£ï¼Œä¹Ÿå°±æ˜¯
*Î£*<sub>*k*</sub>â€„=â€„*Î£*,â€†âˆ€*k*ã€‚

æ¥è‘—ï¼Œæˆ‘å€‘å°±å¯ä»¥ä»¥å»æ¯”è¼ƒå…©å…©é¡åˆ¥ä¹‹é–“çš„å¾Œé©—ç™¼ç”Ÿæ©Ÿç‡*P*(*G*\|*X*)ï¼Œæˆ‘å€‘åœ¨æ­¤åˆ©ç”¨å°æ•¸çš„è‰¯å¥½æ€§è³ªä¾†åˆ†æå…©è€…é—œä¿‚ï¼Œå‡è¨­æˆ‘å€‘ç¾åœ¨è¦æ¢è¨é¡åˆ¥*k*å’Œé¡åˆ¥*l*ï¼Œèª°çš„ç™¼ç”Ÿæ©Ÿç‡å¤§å‘¢ï¼Ÿæˆ‘å€‘ç”¨ä¸‹åˆ—é—œä¿‚å¼ä¾†è¡¨é”ï¼š

$$\\begin{aligned}
\\ln\\frac{P(G=k\|X=x)}{P(G=l\|X=x)}&=\\ln \\frac{f\_k(x)\\pi\_k}{f\_l(x)\\pi\_l}=\\ln\\frac{\\pi\_k}{\\pi\_l}+\\ln {f\_k(x)}-\\ln{f\_l(x)}\\\\
&=\\ln\\frac{\\pi\_k}{\\pi\_l}-\\frac{1}{2}(x-\\mu\_k)^T\\Sigma^{-1}(x-\\mu\_k)+\\frac{1}{2}(x-\\mu\_l)^T\\Sigma^{-1}(x-\\mu\_l)\\\\
&=\\ln\\frac{\\pi\_k}{\\pi\_l}-\\frac{1}{2}(\\mu\_k+\\mu\_l)^T\\Sigma^{-1}(\\mu\_k-\\mu\_l)+x^T\\Sigma^{-1}(\\mu\_k-\\mu\_l)
\\end{aligned}$$

æ³¨æ„!é€™æ¨£è‰¯å¥½çš„ç·šæ€§æ€§è³ªæ˜¯ä¾†è‡ªæ–¼æˆ‘å€‘å‡è¨­å…©å€‹åˆ†é¡å…·æœ‰**ç›¸åŒçš„**å…±è®Šç•°æ•¸çŸ©é™£ã€‚è‹¥æˆ‘å€‘å°é¡åˆ¥*k*å’Œé¡åˆ¥*l*çš„åˆ†ç•Œç·šæ„Ÿèˆˆè¶£ï¼Œå…¶åˆ†ç•Œç·šå°±æ˜¯ä½åœ¨å…©è€…æ©Ÿç‡å¯†åº¦ç›¸ç­‰ä¹‹è™•ï¼Œä¹Ÿå°±æ˜¯ç•¶ä¸Šå¼**ç­‰æ–¼0**æ™‚ã€‚

é€éç›¸åŒçš„æƒ³æ³•ï¼Œæˆ‘å€‘å¯ä»¥å»ºç«‹ä¸€å€‹**ç·šæ€§åˆ¤åˆ¥å‡½æ•¸(linear discriminant
function)***Î´*<sub>*k*</sub>(*x*)ï¼Œä¾†æ±ºå®šè©²è³‡æ–™æ‡‰è¢«åˆ†é…åˆ°å“ªä¸€å€‹é¡åˆ¥ï¼Œä¹Ÿå°±æ˜¯$G(x)=\\mathop{\\arg\\max}\\limits\_{k}\\delta\_k(x)$ã€‚
ç·šæ€§åˆ¤åˆ¥å‡½æ•¸å¦‚ä¸‹æ‰€ç¤ºï¼š
$$\\delta\_k(x)=x^T\\Sigma^{-1}\\mu\_k-\\frac{1}{2}\\mu\_k^T \\Sigma^{-1}\\mu\_k +\\ln \\pi\_k$$

ç·šæ€§åˆ¤åˆ¥å‡½æ•¸çš„æ¨å°ä¾†è‡ªä»¥ä¸‹çš„æˆæ¯”ä¾‹é—œä¿‚ï¼š
$$\\begin{aligned}
P(G=k\|X=x)&\\propto f\_k(x)\\pi\_k \\\\
&\\propto -\\frac{1}{2}(x-\\mu\_k)^T \\Sigma^{-1} (x-\\mu\_k)+\\ln \\pi\_k = -\\frac{1}{2}x^T\\Sigma^{-1}x+x^T\\Sigma^{-1}\\mu\_k-\\frac{1}{2}\\mu\_k^T \\Sigma^{-1}\\mu\_k +\\ln \\pi\_k\\\\
&\\propto  x^T\\Sigma^{-1}\\mu\_k-\\frac{1}{2}\\mu\_k^T \\Sigma^{-1}\\mu\_k +\\ln \\pi\_k \\equiv  \\delta\_k(x)
\\end{aligned}$$

æ­£å¦‚å¤å…¸çš„çµ±è¨ˆåˆ†æä¸€æ¨£ï¼Œæˆ‘å€‘ä¸¦ç„¡æ³•çŸ¥é“æ¯é«”åˆ†é…çš„åƒæ•¸ï¼Œæ•…æˆ‘å€‘åœ¨é€²è¡Œçµ±è¨ˆå­¸ç¿’æ™‚ï¼Œå‰‡é¸æ“‡ä½¿ç”¨**è¨“ç·´é›†çš„è³‡æ–™ä¾†ä¼°è¨ˆæ¯é«”åƒæ•¸**ã€‚

$$\\begin{aligned}
&\\hat{\\pi\_k}=N\_k/N \\\\
 &\\hat{\\mu\_k}=\\sum\_{g\_i=k}x\_i/N\_k \\\\
 &\\hat{\\Sigma}=\\sum\_{k=1}^K\\sum\_{g\_i=k}(x\_i-\\hat{\\mu\_k})(x\_i-\\hat{\\mu\_k})^T/(N-K)
\\end{aligned}$$
å…¶ä¸­ *N*<sub>*k*</sub> æ˜¯é¡åˆ¥*k*åœ¨è¨“ç·´é›†ä¸­çš„æ•¸é‡(observations)ã€‚

### 2-class LDA (from Ex. 4.2)

> Suppose we have features *x*â€„âˆˆâ€„â„<sup>*p*</sup>, a two-class response,
> with class sizes *N*<sub>1</sub>, *N*<sub>2</sub>, and the target
> coded as â€…âˆ’â€…*N*/*N*<sub>1</sub>, *N*/*N*<sub>2</sub>.

> 1.  Show that the LDA rule classifies to class 2 if
>     $$x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu\_2}-\\hat{\\mu\_1}) \> \\frac{1}{2}(\\hat{\\mu\_2}+\\hat{\\mu\_1})^T\\hat{\\Sigma^{-1}}(\\hat{\\mu\_2}-\\hat{\\mu\_1})-\\ln(N\_2/N\_1)$$
>     and class 1 otherwise.

*Sol:*

åœ¨äºŒå…ƒçš„åˆ†é¡ä¸­ï¼Œæˆ‘å€‘å¯ä»¥å›æº¯åˆ°å…ˆå‰è¬›åˆ°çš„log-oddsçš„è§€å¿µï¼Œä¹Ÿå°±æ˜¯å»ºç«‹æ­¤å¼ä¾†åšæ¯”è¼ƒ(æ”¹å¯«è‡ªèª²æœ¬å¼4.9)ï¼š
$$\\begin{aligned}
\\ln\\frac{P(G=2\|X=x)}{P(G=1\|X=x)}
=\\ln\\frac{\\pi\_2}{\\pi\_1}-\\frac{1}{2}(\\mu\_2+\\mu\_1)^T\\Sigma^{-1}(\\mu\_2-\\mu\_1)+x^T\\Sigma^{-1}(\\mu\_2-\\mu\_1)
\\end{aligned}$$
æ ¹æ“šå°æ•¸æ€§è³ªï¼Œè‹¥æ­¤å¼**å¤§æ–¼0**ï¼Œå°±è¡¨ç¤º*P*(*G*â€„=â€„2\|*X*â€„=â€„*x*)çš„æ©Ÿç‡ç›¸å°æ–¼*P*(*G*â€„=â€„1\|*X*â€„=â€„*x*)ä¾†å¾—é«˜ï¼Œæ•…æˆ‘å€‘è‡ªç„¶æœƒå°‡å…¶åˆ†é¡åˆ°
class 2ã€‚
è€Œæˆ‘å€‘æ ¹æ“šè¨“ç·´é›†å°ä¸Šå¼åšä¼°è¨ˆä¸¦ä¸”åšäº›è¨±ä»£æ•¸é‹ç®—å³å¯å¾—åˆ°æ±ºç­–çš„å‡½æ•¸ï¼š
$$x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu\_2}-\\hat{\\mu\_1}) \> \\frac{1}{2}(\\hat{\\mu\_2}+\\hat{\\mu\_1})\\hat{\\Sigma^{-1}}(\\hat{\\mu\_2}-\\hat{\\mu\_1})-\\ln(N\_2/N\_1)$$

> 1.  Consider minimization of the least squares criterion
>     $$\\sum^N\_{i=1}(y\_i-\\beta\_0-x\_i^T\\beta)^2$$
>     Show that the solution *Î²Ì‚* satisfies

$$\[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}\_B\]\\beta=N(\\hat{\\mu\_2}-\\hat{\\mu\_1})$$
where,
$$\\hat{\\Sigma}\_B=\\frac{N\_1N\_2}{N^2}(\\hat{\\mu\_2}-\\hat{\\mu\_1})(\\hat{\\mu\_2}-\\hat{\\mu\_1})^T$$

*Sol:*

é¦–å…ˆï¼Œæˆ‘å€‘çŸ¥é“ *N*â€„=â€„*N*<sub>1</sub>â€…+â€…*N*<sub>2</sub>ã€‚
è€Œé€™å€‹ç·šæ€§è¿´æ­¸æ–¹ç¨‹åŒ…å«å¸¸æ•¸é …ï¼Œå‰‡æ ¹æ“š Normal equation
å¯ä»¥å¾—åˆ°å°$\\hat{\\beta\_0}$å’Œ$\\hat{\\beta\_1}$çš„ä¼°è¨ˆï¼š

$$ \\begin{bmatrix}
\\hat{\\beta\_0}\\\\ 
\\hat{\\beta}
\\end{bmatrix}=(X^TX)^{-1}X^Ty $$

å…¶ä¸­ **designed matrix** å¯é€™æ¨£è¡¨ç¤ºï¼š(*X*è£¡é¢æœ‰$\\bf{1}$å‘é‡)

$$X^TX=\\begin{bmatrix}
N & \\sum\_{i=1}^N x\_i^T\\\\ 
\\sum\_{i=1}^N x\_i & \\sum\_{i=1}^Nx\_ix\_i^T 
\\end{bmatrix}$$
å…¶ä¸­
$$\\sum\_{i=1}^Nx\_i =\\sum\_{i=1}^{N\_1}x\_i+\\sum\_{i=1}^{N\_2}x\_i = N\_1\\hat{\\mu\_1}+ N\_2\\hat{\\mu\_2}$$
åˆçŸ¥
$$\\begin{aligned}
\\hat{\\Sigma} &= \\frac{1}{N-2}\\sum\_{k=1}^2\\sum\_{g\_i=k}(x\_i-\\hat{\\mu\_k})(x\_i-\\hat{\\mu\_k})^T \\\\
&=\\frac{1}{N-2}\[\\sum\_{g\_i=1}x\_ix\_i^T-N\_1 \\mu\_1 \\mu\_1^T+\\sum\_{g\_i=2}x\_ix\_i^T-N\_2\\mu\_2 \\mu\_2^T\]
\\end{aligned}$$
å› æ­¤
$$\\sum\_{i=1}^Nx\_ix\_i^T = (N-2)\\hat{\\Sigma}+N\_1 \\mu\_1 \\mu\_1^T+N\_2\\mu\_2 \\mu\_2^T$$
è‹¥æˆ‘å€‘å°‡å±¬æ–¼é¡åˆ¥1çš„è³‡æ–™æ”¾åœ¨çŸ©é™£çš„å‰*N*<sub>1</sub>å€‹ï¼Œè€Œé¡åˆ¥2çš„è³‡æ–™*N*<sub>2</sub>å€‹ç·Šæ¥åœ¨å¾Œï¼Œ*X*<sup>*T*</sup>*y*å¯ä»¥å¯«æˆï¼š

$$ X^Ty= \\begin{bmatrix}
1 &\\cdots  & 1 &1  & \\cdots &1 \\\\ 
x\_1 &\\cdots  &x\_{N\_1}  &x\_{N\_1+1}  &\\cdots  & x\_{N\_1+N\_2}
\\end{bmatrix}
\\begin{bmatrix}
-N/N\_1\\\\ 
\\vdots\\\\ 
-N/N\_1\\\\ 
N/N\_2\\\\ 
\\vdots\\\\ 
N/N\_2
\\end{bmatrix} =\\begin{bmatrix}
0 \\\\
-N\\mu\_1+N\\mu\_2
\\end{bmatrix}$$

æ•´ç†å…ˆå‰è¨ˆç®—å‡ºçš„é€™äº›æ±è¥¿ä»£å…¥ Normal equationï¼Œæ•´ç†å¾Œå¯å¾—ï¼š

(è¨»ï¼š$\\beta\_0=(-\\frac{N\_1}{N}\\mu\_1^T-\\frac{N\_2}{N}\\mu\_2^T)\\beta$)

$$(N\_1\\mu\_1+N\_2\\mu\_2)(-\\frac{N\_1}{N}\\mu\_1^T-\\frac{N\_2}{N}\\mu\_2^T)\\beta+((N-2)\\hat{\\Sigma} +N\_1\\mu\_1\\mu\_1^T+N\_2\\mu\_2\\mu\_2^T)\\beta=N(\\mu\_2-\\mu\_1)$$

ç¶“éä¸€äº›é‹ç®—ï¼Œä¸¦å¼•å…¥é¡Œç›®å° $\\hat{\\Sigma\_B}$çš„å®šç¾©ï¼Œå³å¯å¾—ï¼š
$$\[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}\_B\]\\beta=N(\\hat{\\mu\_2}-\\hat{\\mu\_1})$$

> 1.  Hence show that *Î£Ì‚*<sub>*B*</sub>*Î²* is in the direction
>     $(\\hat{\\mu\_2} - \\hat{\\mu\_1})$ and thus
>     $$\\hat{\\beta}\\propto \\hat{\\Sigma}^{-1} (\\hat{\\mu\_2} - \\hat{\\mu\_1})$$
>     Therefore the least-squares regression coefficient is identical to
>     the LDA coefficient, up to a scalar multiple.

*Sol:*

è‹¥æˆ‘å€‘ç›´æ¥è¨ˆç®— *Î£Ì‚*<sub>*B*</sub>*Î²*ï¼Œç”±æ–¼
$(\\hat{\\mu\_2}-\\hat{\\mu\_1})^T \\beta$ å…§ç©å¾Œæ˜¯ä¸€å€‹scalarã€‚ä¹Ÿå°±æ˜¯

$$\\hat{\\Sigma}\_B \\beta=\\frac{N\_1N\_2}{N^2}(\\hat{\\mu\_2}-\\hat{\\mu\_1}){\\color{Red} (\\hat{\\mu\_2}-\\hat{\\mu\_1})^T \\beta}=\\frac{N\_1N\_2}{N^2}(\\hat{\\mu\_2}-\\hat{\\mu\_1}){\\color{Red} c}$$

æ›å¥è©±èªªï¼Œ*Î£Ì‚*<sub>*B*</sub>*Î²*å’Œ$(\\hat{\\mu\_2}-\\hat{\\mu\_1})$å·®äº†${\\color{Red} \\frac{N\_1N\_2}{N^2}c}$å€ã€‚åˆé€™äº›å¸¸æ•¸éƒ½ç‚ºæ­£ï¼Œæ•…å¾—çŸ¥ï¼š
$$ \\hat{\\beta} \\propto \\hat{\\Sigma}^{-1} (\\hat{\\mu\_2}-\\hat{\\mu\_1}) $$

æ­¤å³è¡¨ç¤ºç·šæ€§è¿´æ­¸èˆ‡LDAå…¶å¯¦æ˜¯æœ‰ç›¸åƒçš„é‹ç®—é‚è¼¯ã€‚

> 1.  Show that this result holds for any (distinct) coding of the two
>     classes.

*Sol:*

åœ¨ä¸Šä¸€é¡Œä¸­ï¼Œæˆ‘å€‘ä¸¦æ²’æœ‰ç‰¹åˆ¥å° *t*<sub>*k*</sub>
æœ‰ç‰¹åˆ¥å‡è¨­ï¼Œé€™æ¨£çš„æ€§è³ªæ˜¯ä¾†è‡ªæ–¼ä¸€é–‹å§‹å°*Î£Ì‚*<sub>*B*</sub> çš„è¨­è¨ˆï¼Œä¸¦ä¸”èˆ‡
*Î²* å…§ç©å¾Œå‡ºç¾ç´”æ•¸è€Œæ¨æ¼”å‡ºé€™æ¨£çš„æ¯”ä¾‹é—œä¿‚ã€‚

> 1.  Find the solution $\\hat{\\beta\_0}$ (up to the same scalar
>     multiple as in (c), and hence the predicted value
>     $\\hat{f}(x)=\\hat{\\beta\_0}+x^T\\hat{\\beta}$. Consider the
>     following rule: classify to class 2 if *f*(*x*)â€„\>â€„0and class 1
>     otherwise. Show this is **not** the same as the LDA rule unless
>     the classes have equal numbers of observations.

*Sol:*

åœ¨å…ˆå‰çš„è¨è«–ï¼Œæˆ‘å€‘çŸ¥é“ï¼š
$$\\beta\_0 = -\\frac{1}{N}(N\_1\\hat{\\mu}^T\_1+N\_2\\hat{\\mu}^T\_2)\\hat{\\beta}$$

æˆ‘å€‘å¯æ”¹å¯« *fÌ‚*(*x*) å¦‚ä¸‹ï¼š

$$\\begin{aligned}
\\hat{f}(x)&=-\\frac{1}{N}(N\_1\\hat{\\mu}^T\_1+N\_2\\hat{\\mu}^T\_2-Nx^T)    \\hat{\\beta }\\\\
&=-\\frac{1}{N}(N\_1\\hat{\\mu}^T\_1+N\_2\\hat{\\mu}^T\_2-Nx^T) {\\color{Red} \\lambda \\hat{\\Sigma}^{-1}(\\hat{\\mu\_2}-\\hat{\\mu\_1})}
\\end{aligned}$$

ç¶“éå±•é–‹ï¼Œä¸¦é‡æ•´ï¼Œå‰‡å¯å¾—åˆ°ä¸‹å¼ï¼š

$$x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu\_2}-\\hat{\\mu\_1}) \> \\frac{1}{N}(N\_2\\hat{\\mu\_2}+N\_1\\hat{\\mu\_1})^T\\hat{\\Sigma^{-1}}(\\hat{\\mu\_2}-\\hat{\\mu\_1})$$
è‹¥ *N*<sub>1</sub>â€„=â€„*N*<sub>2</sub>
ï¼Œå‰‡*N*<sub>1</sub>/*N*â€„=â€„*N*<sub>2</sub>/*N*â€„=â€„1/2ï¼Œè€Œlnâ€†(*N*<sub>2</sub>/*N*<sub>1</sub>)â€„=â€„lnâ€†1â€„=â€„0

é€™æ¨£çš„æ±ºç­–æº–å‰‡å°±è·ŸLDAä¸€æ¨£äº†ï¼

Quadratic Discriminant Analysis (QDA)
-------------------------------------

é‚„è¨˜å¾—å…ˆå‰åœ¨æ¨å° LDA
æ™‚é™¤äº†å¸¸æ…‹å‡è¨­ä¹‹å¤–ï¼Œé‚„æœ‰ä¸€å€‹è®Šç•°æ•¸çŸ©é™£ç›¸ç­‰çš„å‡è¨­ï¼Œè‹¥æˆ‘å€‘æ”¾å¯¬é€™å€‹å‡è¨­ï¼Œé€™å€‹æ–¹æ³•å°±è®Šæˆ
QDA ï¼ŒQuadratic
çš„éƒ¨åˆ†å°±æ˜¯ä¾†è‡ªæ–¼å¤šè®Šé‡å¸¸æ…‹åˆ†é…ä¸­åœ¨*e*ä¸Šçš„*x*äºŒæ¬¡å¼ä¸æœƒè¢«å°æ¶ˆã€‚

äºŒæ¬¡åˆ¤åˆ¥å‡½æ•¸(Quadratic Discriminant function)å¦‚ä¸‹ï¼š

$$\\delta\_k(x)=-\\frac{1}{2}\\ln\|\\Sigma\_k\|-\\frac{1}{2}(x-\\mu\_k)^T\\Sigma\_k^{-1}(x-\\mu\_k)+\\ln \\pi\_k$$

å€‹é¡åˆ¥çš„åˆ†ç•Œç·šå¯ä»¥é€™æ¨£è¡¨ç¤º

{*x*\|*Î´*<sub>*k*</sub>(*x*)â€„=â€„*Î´*<sub>*l*</sub>(*x*)}
QDA æœ‰åƒæ•¸å¤ªå¤šçš„ç¼ºé»ã€‚

Regulized Discriminant Analysis (RDA)
-------------------------------------

![](https://esl.hohoweiya.xyz/img/04/fig4.7.png)

123
