<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.245">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>統計學習理論 – sec4.3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">統計學習理論</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../images/statlearn.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">統計學習理論</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../Ch4/Sec4.2.html" class="sidebar-item-text sidebar-link">Chapter 2.</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch2/Sec2.3.html" class="sidebar-item-text sidebar-link">2.3 Two Simple Approaches to Prediction</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Chapter 4.</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch4/Sec4.2.html" class="sidebar-item-text sidebar-link">4.2 Linear Regression of an Indicator Matrix</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch4/Sec4.3.html" class="sidebar-item-text sidebar-link active">4.3 Linear Discriminant Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch4/Sec4.4.html" class="sidebar-item-text sidebar-link">4.4 Logistic Regression</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Chapter 5.</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch5/Sec5.1.html" class="sidebar-item-text sidebar-link">5.1 Introduction</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch5/Sec5.2.html" class="sidebar-item-text sidebar-link">5.2 Piecewise Polynomials and Spl</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch5/Sec5.4.html" class="sidebar-item-text sidebar-link">5.4 Smoothing Splines</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch5/Sec5.5.html" class="sidebar-item-text sidebar-link">5.5 Automatic Selection of the Smoothing Parameters</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Chapter 14.</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Ch14/Sec14.5.html" class="sidebar-item-text sidebar-link">14.5 Principal Components, Curves and Surfaces</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#linear-discriminant-analysislda" id="toc-linear-discriminant-analysislda" class="nav-link active" data-scroll-target="#linear-discriminant-analysislda">4.3 Linear Discriminant Analysis(LDA)</a>
  <ul class="collapse">
  <li><a href="#class-lda-from-ex.-4.2" id="toc-class-lda-from-ex.-4.2" class="nav-link" data-scroll-target="#class-lda-from-ex.-4.2">2-class LDA (from Ex. 4.2)</a></li>
  </ul></li>
  <li><a href="#quadratic-discriminant-analysis-qda" id="toc-quadratic-discriminant-analysis-qda" class="nav-link" data-scroll-target="#quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis (QDA)</a></li>
  <li><a href="#regulized-discriminant-analysis-rda" id="toc-regulized-discriminant-analysis-rda" class="nav-link" data-scroll-target="#regulized-discriminant-analysis-rda">Regulized Discriminant Analysis (RDA)</a></li>
  <li><a href="#lda-的計算" id="toc-lda-的計算" class="nav-link" data-scroll-target="#lda-的計算">LDA 的計算</a></li>
  <li><a href="#reduced-rank-lda" id="toc-reduced-rank-lda" class="nav-link" data-scroll-target="#reduced-rank-lda">Reduced-Rank LDA</a></li>
  <li><a href="#lda-和-linear-regression-的關係-from-ex.-4.3" id="toc-lda-和-linear-regression-的關係-from-ex.-4.3" class="nav-link" data-scroll-target="#lda-和-linear-regression-的關係-from-ex.-4.3">LDA 和 linear regression 的關係 (from Ex. 4.3)</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.dev/wwwh0225/elements-of-statistical-learning/blob/main/Ch4/Sec4.3.ipynb" class="toc-action">Edit this page</a></p><p><a href="https://github.com/wwwh0225/elements-of-statistical-learning/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<p><a href="https://colab.research.google.com/github/wwwh0225/elements-of-statistical-learning/blob/main/Ch4.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<section id="linear-discriminant-analysislda" class="level2">
<h2 class="anchored" data-anchor-id="linear-discriminant-analysislda">4.3 Linear Discriminant Analysis(LDA)</h2>
<p>在課本的2.4節，我們知道在做分類決策時，我們是在極大化某種後驗機率(posrerior probability)，也就給定 <span class="math inline">\(X=x\)</span> 時，找一個最大可能性的分類當作分析的結果。 也就是說： 我們令分類所做的預測 <span class="math inline">\(\hat{G}(x)=\mathcal{G}_k\)</span> ( <span class="math inline">\(\mathcal{G}_k\)</span> 為其中一種分類)； 也就表示，當給定 <span class="math inline">\(X=x\)</span> 的條件機率之下， <span class="math inline">\(\mathcal{G}_k\)</span> 是最有可能的出象，換成數學的語言就是： <span class="math inline">\(P(\mathcal{G}_k|X=x)=\max_{l}P(G=\mathcal{G}_l|X=x)\)</span><br>
（或者說是 <span class="math inline">\(=\max_{l}P(G=l|X=x)\)</span> ）</p>
<p>有這個基本觀念後，我們設定資料屬於類別 <span class="math inline">\(k\)</span> 的先驗機率為： <span class="math inline">\(\pi_k=P(G=k)\)</span> ，而當然 <span class="math inline">\(\sum_{k=1}^K\pi_k =1\)</span>。</p>
<p>而透過貝氏定理，我們可以得到以下關係：</p>
<p><span class="math display">\[P(G=k|X=x)=\frac{{\color{Red} f_k(x)}{\color{Blue} \pi_k}}{{\color{DarkOrange} \Sigma_{l=1}^k f_l(x)\pi_l}}=\frac{{\color{Red} P(X|G=k)}{\color{Blue} P(G=k)} }{{\color{DarkOrange} \Sigma_l P(X|G=l)P(G=l)}}\]</span></p>
<p>根據上式，我們必須對 <span class="math inline">\(f_k(x)\)</span> 做一些假設，這也就是當資料是第 <span class="math inline">\(k\)</span> 類時， <span class="math inline">\(X\)</span> 的機率密度函數。我們設定各分類的機率密度服從「<strong>多變量常態分配(Multivariate Normal Distribution)</strong>」。</p>
<p><span class="math display">\[f_k(x)=\frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^\frac{1}{2}}e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}\]</span></p>
<p>要注意的是，在LDA的架構之下，所有類別的pdf均享有相同的共變數矩陣，也就是 <span class="math inline">\(\Sigma_k=\Sigma,\forall k\)</span>。</p>
<p>接著，我們就可以以去比較兩兩類別之間的後驗發生機率 <span class="math inline">\(P(G|X)\)</span> ，我們在此利用對數的良好性質來分析兩者關係，假設我們現在要探討類別 <span class="math inline">\(k\)</span> 和類別 <span class="math inline">\(l\)</span> ，誰的發生機率大呢？我們用下列關係式來表達：</p>
<p><span class="math display">\[\begin{aligned}
\ln\frac{P(G=k|X=x)}{P(G=l|X=x)}&amp;=\ln \frac{f_k(x)\pi_k}{f_l(x)\pi_l}=\ln\frac{\pi_k}{\pi_l}+\ln {f_k(x)}-\ln{f_l(x)}\\
&amp;=\ln\frac{\pi_k}{\pi_l}-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)+\frac{1}{2}(x-\mu_l)^T\Sigma^{-1}(x-\mu_l)\\
&amp;=\ln\frac{\pi_k}{\pi_l}-\frac{1}{2}(\mu_k+\mu_l)^T\Sigma^{-1}(\mu_k-\mu_l)+x^T\Sigma^{-1}(\mu_k-\mu_l)
\end{aligned}\]</span></p>
<p>注意!這樣良好的線性性質是來自於我們假設兩個分類具有<strong>相同的</strong>共變異數矩陣。若我們對類別 <span class="math inline">\(k\)</span> 和類別 <span class="math inline">\(l\)</span> 的分界線感興趣，其分界線就是位在兩者機率密度相等之處，也就是當上式<strong>等於0</strong>時。</p>
<p>透過相同的想法，我們可以建立一個<strong>線性判別函數(linear discriminant function)</strong> <span class="math inline">\(\delta_k(x)\)</span> ，來決定該資料應被分配到哪一個類別，也就是 <span class="math inline">\(G(x)=\mathop{\arg\max}\limits_{k}\delta_k(x)\)</span> 。 線性判別函數如下所示：</p>
<p><span class="math display">\[\delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k +\ln \pi_k\]</span></p>
<p>線性判別函數的推導來自以下的成比例關係：</p>
<p><span class="math display">\[\begin{aligned}
P(G=k|X=x)&amp;\propto f_k(x)\pi_k \\
&amp;\propto -\frac{1}{2}(x-\mu_k)^T \Sigma^{-1} (x-\mu_k)+\ln \pi_k = -\frac{1}{2}x^T\Sigma^{-1}x+x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k +\ln \pi_k\\
&amp;\propto  x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k +\ln \pi_k \equiv  \delta_k(x)
\end{aligned}\]</span></p>
<p>正如古典的統計分析一樣，我們並無法知道母體分配的參數，故我們在進行統計學習時，則選擇使用<strong>訓練集的資料來估計母體參數</strong>。</p>
<p><span class="math display">\[\begin{aligned}
&amp;\hat{\pi_k}=N_k/N \\
&amp;\hat{\mu_k}=\sum_{g_i=k}x_i/N_k \\
&amp;\hat{\Sigma}=\sum_{k=1}^K\sum_{g_i=k}(x_i-\hat{\mu_k})(x_i-\hat{\mu_k})^T/(N-K)
\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(N_k\)</span> 是類別<span class="math inline">\(k\)</span>在訓練集中的數量(observations)。</p>
<hr>
<section id="class-lda-from-ex.-4.2" class="level3">
<h3 class="anchored" data-anchor-id="class-lda-from-ex.-4.2">2-class LDA (from Ex. 4.2)</h3>
<p><strong>Suppose we have features <span class="math inline">\(x \in \mathbb{R}^p\)</span>, a two-class response, with class sizes <span class="math inline">\(N_1\)</span>, <span class="math inline">\(N_2\)</span>, and the target coded as <span class="math inline">\(−N/N_1\)</span>, <span class="math inline">\(N/N_2\)</span>.</strong></p>
<p><strong>(a) Show that the LDA rule classifies to class 2 if</strong> <span class="math display">\[x^T\hat{\Sigma^{-1}}(\hat{\mu_2}-\hat{\mu_1}) &gt; \frac{1}{2}(\hat{\mu_2}+\hat{\mu_1})^T\hat{\Sigma^{-1}}(\hat{\mu_2}-\hat{\mu_1})-\ln(N_2/N_1)\]</span> <strong>and class 1 otherwise.</strong></p>
<p><em>Sol:</em></p>
<p>在二元的分類中，我們可以回溯到先前講到的log-odds的觀念，也就是建立此式來做比較(改寫自課本式4.9)：</p>
<p><span class="math display">\[\begin{aligned}
\ln\frac{P(G=2|X=x)}{P(G=1|X=x)}
=\ln\frac{\pi_2}{\pi_1}-\frac{1}{2}(\mu_2+\mu_1)^T\Sigma^{-1}(\mu_2-\mu_1)+x^T\Sigma^{-1}(\mu_2-\mu_1)
\end{aligned}\]</span></p>
<p>根據對數性質，若此式<strong>大於0</strong>，就表示 <span class="math inline">\(P(G=2|X=x)\)</span> 的機率相對於 <span class="math inline">\(P(G=1|X=x)\)</span> 來得高，故我們自然會將其分類到 class 2。 而我們根據訓練集對上式做估計並且做些許代數運算即可得到決策的函數： <span class="math display">\[x^T\hat{\Sigma^{-1}}(\hat{\mu_2}-\hat{\mu_1}) &gt; \frac{1}{2}(\hat{\mu_2}+\hat{\mu_1})\hat{\Sigma^{-1}}(\hat{\mu_2}-\hat{\mu_1})-\ln(N_2/N_1)\]</span></p>
<p><strong>(b) Consider minimization of the least squares criterion</strong> <span class="math display">\[\sum^N_{i=1}(y_i-\beta_0-x_i^T\beta)^2\]</span></p>
<p><strong>Show that the solution <span class="math inline">\(\hat{\beta}\)</span> satisfies</strong></p>
<p><span class="math display">\[[(N-2)\hat{\Sigma}+N\hat{\Sigma}_B]\beta=N(\hat{\mu_2}-\hat{\mu_1})\]</span></p>
<p>where, <span class="math display">\[\hat{\Sigma}_B=\frac{N_1N_2}{N^2}(\hat{\mu_2}-\hat{\mu_1})(\hat{\mu_2}-\hat{\mu_1})^T\]</span></p>
<p><em>Sol:</em></p>
<p>首先，我們知道 <span class="math inline">\(N=N_1+N_2\)</span>。 而這個線性迴歸方程包含常數項，則根據 Normal equation 可以得到對 <span class="math inline">\(\hat{\beta_0}\)</span> 和 <span class="math inline">\(\hat{\beta_1}\)</span> 的估計：</p>
<p><span class="math display">\[ \begin{bmatrix}
\hat{\beta_0}\\
\hat{\beta}
\end{bmatrix}=(X^TX)^{-1}X^Ty \]</span></p>
<p>其中 <strong>designed matrix</strong> 可這樣表示：( <span class="math inline">\(X\)</span> 裡面有 <span class="math inline">\(\bf{1}\)</span> 向量)</p>
<p><span class="math display">\[X^TX=\begin{bmatrix}
N &amp; \sum_{i=1}^N x_i^T\\
\sum_{i=1}^N x_i &amp; \sum_{i=1}^Nx_ix_i^T
\end{bmatrix}\]</span></p>
<p>其中</p>
<p><span class="math display">\[\sum_{i=1}^Nx_i =\sum_{i=1}^{N_1}x_i+\sum_{i=1}^{N_2}x_i = N_1\hat{\mu_1}+ N_2\hat{\mu_2}\]</span></p>
<p>又知</p>
<p><span class="math display">\[\begin{aligned}
\hat{\Sigma} &amp;= \frac{1}{N-2}\sum_{k=1}^2\sum_{g_i=k}(x_i-\hat{\mu_k})(x_i-\hat{\mu_k})^T \\
&amp;=\frac{1}{N-2}[\sum_{g_i=1}x_ix_i^T-N_1 \mu_1 \mu_1^T+\sum_{g_i=2}x_ix_i^T-N_2\mu_2 \mu_2^T]
\end{aligned}\]</span></p>
<p>因此</p>
<p><span class="math display">\[\sum_{i=1}^Nx_ix_i^T = (N-2)\hat{\Sigma}+N_1 \mu_1 \mu_1^T+N_2\mu_2 \mu_2^T\]</span></p>
<p>若我們將屬於類別1的資料放在矩陣的前 <span class="math inline">\(N_1\)</span> 個，而類別2的資料 <span class="math inline">\(N_2\)</span> 個緊接在後，<span class="math inline">\(X^Ty\)</span> 可以寫成：</p>
<p><span class="math display">\[ X^Ty= \begin{bmatrix}
1 &amp;\cdots  &amp; 1 &amp;1  &amp; \cdots &amp;1 \\
x_1 &amp;\cdots  &amp;x_{N_1}  &amp;x_{N_1+1}  &amp;\cdots  &amp; x_{N_1+N_2}
\end{bmatrix}
\begin{bmatrix}
-N/N_1\\
\vdots\\
-N/N_1\\
N/N_2\\
\vdots\\
N/N_2
\end{bmatrix} =\begin{bmatrix}
0 \\
-N\mu_1+N\mu_2
\end{bmatrix}\]</span></p>
<p>整理先前計算出的這些東西代入 Normal equation，整理後可得：</p>
<p>(註： <span class="math inline">\(\beta_0=(-\frac{N_1}{N}\mu_1^T-\frac{N_2}{N}\mu_2^T)\beta\)</span> )</p>
<p><span class="math display">\[(N_1\mu_1+N_2\mu_2)(-\frac{N_1}{N}\mu_1^T-\frac{N_2}{N}\mu_2^T)\beta+((N-2)\hat{\Sigma} +N_1\mu_1\mu_1^T+N_2\mu_2\mu_2^T)\beta=N(\mu_2-\mu_1)\]</span></p>
<p>經過一些運算，並引入題目對 <span class="math inline">\(\hat{\Sigma_B}\)</span> 的定義，即可得：</p>
<p><span class="math display">\[[(N-2)\hat{\Sigma}+N\hat{\Sigma}_B]\beta=N(\hat{\mu_2}-\hat{\mu_1})\]</span></p>
<p><strong>(c) Hence show that <span class="math inline">\(\hat{\Sigma}_B \beta\)</span> is in the direction <span class="math inline">\((\hat{\mu_2} - \hat{\mu_1})\)</span> and thus</strong></p>
<p><span class="math display">\[\hat{\beta}\propto \hat{\Sigma}^{-1} (\hat{\mu_2} - \hat{\mu_1})\]</span></p>
<p><strong>Therefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.</strong></p>
<p><em>Sol:</em></p>
<p>若我們直接計算 <span class="math inline">\(\hat{\Sigma}_B \beta\)</span> ，由於 <span class="math inline">\((\hat{\mu_2}-\hat{\mu_1})^T \beta\)</span> 內積後是一個scalar。也就是</p>
<p><span class="math display">\[\hat{\Sigma}_B \beta=\frac{N_1N_2}{N^2}(\hat{\mu_2}-\hat{\mu_1}){\color{Red} (\hat{\mu_2}-\hat{\mu_1})^T \beta}=\frac{N_1N_2}{N^2}(\hat{\mu_2}-\hat{\mu_1}){\color{Red} c}\]</span></p>
<p>換句話說， <span class="math inline">\(\hat{\Sigma}_B \beta\)</span> 和 <span class="math inline">\((\hat{\mu_2}-\hat{\mu_1})\)</span> 差了 <span class="math inline">\({ \frac{N_1N_2}{N^2}c}\)</span> 倍。又這些常數都為正，故得知： <span class="math display">\[ \hat{\beta} \propto \hat{\Sigma}^{-1} (\hat{\mu_2}-\hat{\mu_1}) \]</span></p>
<p>此即表示線性迴歸與LDA其實是有相像的運算邏輯。</p>
<p><strong>(d) Show that this result holds for any (distinct) coding of the two classes.</strong></p>
<p><em>Sol:</em></p>
<p>在上一題中，我們並沒有特別對 <span class="math inline">\(t_k\)</span> 有特別假設，這樣的性質是來自於一開始對 <span class="math inline">\(\hat{\Sigma}_B\)</span> 的設計，並且與 <span class="math inline">\(\beta\)</span> 內積後出現純數而推演出這樣的比例關係。</p>
<p><strong>(e) Find the solution <span class="math inline">\(\hat{\beta_0}\)</span> (up to the same scalar multiple as in (c), and hence the predicted value <span class="math inline">\(\hat{f}(x)=\hat{\beta_0}+x^T\hat{\beta}\)</span>. Consider the following rule: classify to class 2 if <span class="math inline">\(f(x) &gt; 0\)</span>and class 1 otherwise. Show this is not the same as the LDA rule unless the classes have equal numbers of observations.</strong></p>
<p><em>Sol:</em></p>
<p>在先前的討論，我們知道：</p>
<p><span class="math display">\[\beta_0 = -\frac{1}{N}(N_1\hat{\mu}^T_1+N_2\hat{\mu}^T_2)\hat{\beta}\]</span></p>
<p>我們可改寫 <span class="math inline">\(\hat{f}(x)\)</span> 如下：</p>
<p><span class="math display">\[\begin{aligned}
\hat{f}(x)&amp;=-\frac{1}{N}(N_1\hat{\mu}^T_1+N_2\hat{\mu}^T_2-Nx^T)    \hat{\beta }\\
&amp;=-\frac{1}{N}(N_1\hat{\mu}^T_1+N_2\hat{\mu}^T_2-Nx^T) {\color{Red} \lambda \hat{\Sigma}^{-1}(\hat{\mu_2}-\hat{\mu_1})}
\end{aligned}\]</span></p>
<p>經過展開，並重整，則可得到下式：</p>
<p><span class="math display">\[x^T\hat{\Sigma^{-1}}(\hat{\mu_2}-\hat{\mu_1}) &gt; \frac{1}{N}(N_2\hat{\mu_2}+N_1\hat{\mu_1})^T\hat{\Sigma^{-1}}(\hat{\mu_2}-\hat{\mu_1})\]</span> 若 <span class="math inline">\(N_1=N_2\)</span> ，則 <span class="math inline">\(N_1/N=N_2/N=1/2\)</span>，而 <span class="math inline">\(\ln(N_2/N_1)=\ln 1=0\)</span></p>
<p>這樣的決策準則就跟LDA一樣了！</p>
</section>
</section>
<section id="quadratic-discriminant-analysis-qda" class="level2">
<h2 class="anchored" data-anchor-id="quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis (QDA)</h2>
<p>還記得先前在推導 LDA 時除了常態假設之外，還有一個變異數矩陣相等的假設，若我們放寬這個假設，這個方法就變成 QDA ，Quadratic 的部分就是來自於多變量常態分配中在 <span class="math inline">\(e\)</span> 上的 <span class="math inline">\(x\)</span> 二次式不會被對消。</p>
<p>二次判別函數(Quadratic Discriminant function)如下：</p>
<p><span class="math display">\[\delta_k(x)=-\frac{1}{2}\ln|\Sigma_k|-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)+\ln \pi_k\]</span></p>
<p><span class="math inline">\(l\)</span> 個類別的分界線可以這樣表示</p>
<p><span class="math display">\[\{x|\delta_k(x)=\delta_l(x) \}\]</span></p>
<p>QDA 有參數太多的缺點。</p>
</section>
<section id="regulized-discriminant-analysis-rda" class="level2">
<h2 class="anchored" data-anchor-id="regulized-discriminant-analysis-rda">Regulized Discriminant Analysis (RDA)</h2>
<p><img src="https://esl.hohoweiya.xyz/img/04/fig4.7.png" class="img-fluid"></p>
<p>Friedman(1989) 提出了介於LDA與QDA之間的方法，稱之為 <strong>Regulized Discriminant Analysis (RDA)</strong>。正則化的共變異數矩陣如下所示：</p>
<p><span class="math display">\[\hat{\Sigma}_k(\alpha)=\alpha \hat{\Sigma}_k+(1-\alpha)\hat{\Sigma},\ \alpha \in[0,1]\]</span></p>
<p>在此， <span class="math inline">\(\hat{\Sigma}\)</span> 是LDA所做的共變數矩陣，而 <span class="math inline">\(\hat{\Sigma}_k\)</span> 為QDA的共變數矩陣。</p>
<p>by 課本</p>
<p><span class="math display">\[\hat{\Sigma}(\gamma)=\gamma\hat{\Sigma}+(1-\gamma)\hat{\sigma}^2I\]</span></p>
</section>
<section id="lda-的計算" class="level2">
<h2 class="anchored" data-anchor-id="lda-的計算">LDA 的計算</h2>
</section>
<section id="reduced-rank-lda" class="level2">
<h2 class="anchored" data-anchor-id="reduced-rank-lda">Reduced-Rank LDA</h2>
</section>
<section id="lda-和-linear-regression-的關係-from-ex.-4.3" class="level2">
<h2 class="anchored" data-anchor-id="lda-和-linear-regression-的關係-from-ex.-4.3">LDA 和 linear regression 的關係 (from Ex. 4.3)</h2>
<p><strong>Suppose that we transform the original predictors <span class="math inline">\(\mathbf{X}\)</span> to <span class="math inline">\(\mathbf{\hat{Y}}\)</span> by taking the predicted values under linear regression. Show that LDA using <span class="math inline">\(\bf{\hat{Y}}\)</span> is identical to using LDA in the original space.</strong></p>
<p><em>Sol:</em> 根據題意，我們知道 <span class="math inline">\(x \in \mathbb{R}^p\)</span> 和 <span class="math inline">\(y \in \mathbb{R}^k\)</span> ，且：</p>
<p><span class="math display">\[\begin{aligned}
&amp; \hat{y}=\hat{\mathbf{B}}x \\
&amp; \mathbf{\hat{Y}} = \mathbf{X} \hat{\mathbf{B}}= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\end{aligned}\]</span></p>
<p>對於任意的類別 <span class="math inline">\(k\)</span> ，我們可以建構出該類別的平均數，我們定義類別 <span class="math inline">\(k\)</span> 對應其 <span class="math inline">\(X\)</span> 以及 <span class="math inline">\(Y\)</span> 的平均數為：</p>
<p><span class="math display">\[\begin{aligned}
&amp; \mu_k = \frac{1}{N_k}\sum_{g_i=k}x^T_i \\
&amp; \hat{\mu_k} = \mathbf{B}^T\mu_k
\end{aligned}\]</span></p>
<p>若我們定義 <span class="math inline">\(\mathbf{X}\)</span> 的共變數矩陣為 <span class="math inline">\({\Sigma}\)</span> ，則 <span class="math inline">\(\hat{\mathbf{Y}}\)</span> 的共變數矩陣則為：</p>
<p><span class="math display">\[\hat{\Sigma} = \mathbf{B}^T \Sigma \mathbf{B}\]</span></p>
<p>其中</p>
<p><span class="math display">\[\begin{aligned}
&amp; \Sigma =\frac{1}{N-K}X^T(I-YD^{-1}Y^T)X\\
&amp;,where\  D =\begin{bmatrix}
n_1 &amp; 0  &amp; \cdots &amp;0 \\
0 &amp; n_2 &amp;\cdots  &amp;0 \\
\vdots&amp;\vdots  &amp; \vdots &amp; \vdots\\
0 &amp;0  &amp; \cdots &amp; n_k
\end{bmatrix}
\end{aligned}\]</span></p>
<p>若我們直接用 <span class="math inline">\(\hat{\mathbf{Y}}\)</span> 的資料做 LDA，則判別函數為：</p>
<p><span class="math display">\[\delta_k(\hat{Y})= \hat{\mathbf{Y}}\hat{\Sigma}^{-1}\hat{\mu_k}-\frac{1}{2}\hat{\mu_k}^T\hat{\Sigma}^{-1}\hat{\mu_k}+\ln \pi_k\]</span></p>
<p>第一項可以改寫如下(全部寫成矩陣的樣子)：( <span class="math inline">\(B=(X^TX)^{-1}X^TY\)</span> )</p>
<p><span class="math display">\[\begin{aligned}
\hat{\mathbf{Y}}\hat{\Sigma}^{-1}\hat{\mu}&amp;=(XB)(B^T\Sigma B)^{-1}(B^TX^TYD^{-1}) \\
&amp;=X\hat{\Sigma}^{-1}\mu
\end{aligned}\]</span></p>
<p>這就是使用 <span class="math inline">\(\mathbf{X}\)</span> 進行LDA的其中一部分。</p>
<p>而第二項，若將全部 <span class="math inline">\(K\)</span> 種類的平均搜集起來，寫成矩陣 <span class="math inline">\(\hat{\mu}\)</span> 可改寫為：</p>
<p><span class="math display">\[\begin{aligned}
\hat{\mu_k}^T\hat{\Sigma}^{-1}\hat{\mu}&amp;=(B^T \mu_k)^T\hat{\Sigma}^{-1}(B^TX^TYD^{-1})\\
&amp;=\mu_k^TB\hat{\Sigma}^{-1}(B^TX^TYD^{-1})\\
&amp;=\mu_k^TB(B^T\Sigma B)^{-1}B^TX^TYD^{-1}\\
&amp;=\mu_k^T\Sigma^{-1}\mu
\end{aligned}\]</span></p>
<p>此即使用 <span class="math inline">\(\mathbf{X}\)</span> 做LDA的另外一部分。 所以我們可以知道，若我們透過迴歸方程的模式找出 <span class="math inline">\(\hat{\mathbf{Y}}\)</span> ，我們便可從對 <span class="math inline">\(\hat{\mathbf{Y}}\)</span> 做LDA轉換成對 <span class="math inline">\(\mathbf{X}\)</span> 做LDA。</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-right">Powered by <a href="https://quarto.org/"><img src="https://quarto.org/quarto.png" class="img-fluid" alt="Quarto" width="65"></a></div>
  </div>
</footer>



</body></html>