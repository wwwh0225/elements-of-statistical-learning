[
  {
    "objectID": "Ch14/Sec14.5.html",
    "href": "Ch14/Sec14.5.html",
    "title": "統計學習理論",
    "section": "",
    "text": "14.5.1 Principle Components 主成分(分析)\n\n1. 概念\n\n這章節對於主成分的刻畫，採用了 Pearson 的想法：在一個小於 \\(p\\) 維的低維 (\\(q\\)維) 的空間中，尋找原本資料的線性近似。\n\n我們可以把原本資料的線性近似表達成以下的形式：\n\\[\\begin{align}\nf(\\lambda) &= \\mu + V_q\\lambda \\tag{14.49}\n\\end{align}\\]\n其中 \\(\\mu\\) 是這個 \\(q\\) 維空間的原點。\\(V_q\\) 則為 \\(p \\times q\\) 的矩陣，代表 \\(q\\) 個用來展開這個低維空間的 othonormal 向量 (因此 \\(V_q^TV_q=I_q\\))。\n我們的目標既然是去近似原本空間的 \\(p\\) 維向量，尋找合理的 \\(f(\\lambda)\\) 來最小化原本資料和低維對應之間的近似誤差 (reconstruction error) 是件很自然的事：\n\\[\\begin{align*}\n\\min_{\\mu, \\{\\lambda_i\\}, V_q} \\quad & \\sum_{i=1}^N \\|x_i-f(\\lambda_i)\\|^2  \\\\\n=& \\sum_{i=1}^N \\|x_i - \\mu - V_q\\lambda_i\\|^2 \\tag{14.50}\n\\end{align*}\\]\n\n\n2. 推導 (14.51) (14.52)\n以下內容完成了 Exercise 14.7\n\n假使我們已經選好一個 \\(q\\) 維空間準備去近似原本的資料點 (換句話說，空間原點 \\(\\mu\\) 和 空間基底向量 \\(V_q\\) 已知)，則令 \\(y_i=x_i-\\mu\\)，則 (14.50) 可以改寫為我們熟悉的 least square 問題：\n\n\\[\n\\min_{\\{\\lambda_i\\}} \\sum_{i=1}^N \\|y_i - V_q\\lambda_i\\|^2 \\quad\\Leftrightarrow\\quad \\min_{\\lambda_i} \\|y_i - V_q\\lambda_i\\|^2\n\\]\n所以 \\(\\lambda_i = (V_q^TV_q)^{-1}V_q^Ty_i = V_q^T(x_i-\\mu)\\)。\n(第二個等式成立是因為每個 \\(V_q\\) 中的行向量是一組 orthonormal 的向量，所以 \\(V_q^TV_q=I_q\\))\n\n\n定義 \\(H\\equiv V_qV_q^T\\)，由 (1) 可知 \\(q\\) 維空間中的線性近似 \\(f(\\lambda) = \\mu + V_qV_q^T(x_i-\\mu) = \\mu + H(x_i-\\mu)\\)。我們接著找出在給定空間基底 \\(V_q\\) 之下，使近似差異最小的空間原點 \\(\\mu\\)。\n\n我們將 \\(f(\\lambda)\\) 代入原問題，並目標式由總和調整為平均(因為只在前面乘上一個常數，所以不會改變最佳解)：\n\\[\\begin{align*}\n\\min_{\\mu} & \\frac{1}{N}\\sum_{i=1}^N \\|(I-H)(x_i-\\mu)\\|^2 \\\\\n=& \\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^T(I-H)^T(I-H)(x_i-\\mu)  \\\\\n=& \\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^T(I-H)(x_i-\\mu) \\\\\n=& \\left[\\frac{1}{N}\\sum_{i=1}^N x_i^T(I-H)x_i \\right] -\\mu^T(I-H)\\bar{x} - \\bar{x}^T(I-H)\\mu + \\mu^T(I-H)\\mu\n\\end{align*}\\]\n其中 \\(\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i\\)。\n因此對 \\(\\mu\\) 而言，原問題為一個 quadratic programming 問題 (\\(I-H\\) 為 semi-positive definite)，對 \\(\\mu\\) 求導能得到上述問題的一階條件：\n\\[\n(I-H)\\mu = (I-H)\\bar{x}\n\\]\n我們可以選擇 \\(\\mu =\\bar{x}\\)。但注意到 \\((I-H)\\) 不是 full-rank 的矩陣，所以有無窮多組解使前述的一階條件成立。\n\\((I-H)\\) 作為 \\(H=V_qV_q^T\\) 的正交投影，\\(rank(V_qV_q^T) = rank(V_q) = q\\)，所以 \\((I-H)\\) 的零空間有 \\(q\\) 個基底。\n因此一階條件的解集合 \\(S\\) 為\n\\[\nS = \\{\\mu| \\mu=\\bar{x}+\\sum_{i=1}^{q}a_iv_i\\}\n\\]\n換句話說，任何跟 \\(\\bar{x}\\) 處在同一個 subspace 的向量，都可以作為展開這個空間的出發點。\n\n\n由 (1) 和 (2) 我們可以得到：\n\n\\[\\begin{align*}\n\\mu &= \\bar{x} \\tag{14.51} \\\\\n\\lambda_i &= V_q^T(x_i-\\bar{x}) \\tag{14.52}\n\\end{align*}\\]\n\\(\\mathrm{\\square}\\)\n\n\n3. 推導(14.53) (14.54)：解出\\(V_q\\)\n以下內容說明為什麼要對 \\(X\\) 做 SVD。\n由前面我們知道，給定空間(即給定原點、基底)我們可以得到原空間每個點於該低維空間的最佳近似；給定基底向量，我們可以知道原點該如何設定(在空間中的任何一點皆可)。那麼我們只要決定基底向量，就可以延續前面的推論將近似點找出來。這個問題的關鍵，於是成為「如何找到一組基底向量 \\(V_q\\) 使目標式最小」。我們可以將 (14.51) 和 (14.52) 代入目標式：\n不失一般性，我們假設 \\(\\bar{x} = 0\\)。則原式\n\\[\\begin{align*}\n\\min_{V_q} \\ & \\sum_{i=1}^N \\|(x_i-\\bar{x})-V_qV_q^T(x_i-\\bar{x}) \\|^2 \\tag{14.53}\\\\\n=& \\sum_{i=1}^N \\|x_i-V_qV_q^Tx_i \\|^2 \\\\\n=& \\sum_{i=1}^N \\|(I-V_qV_q^T)x_i \\|^2\n\\end{align*}\\]\n將標準化後的樣本 \\(x_i - \\bar{x}\\) 堆疊成一個 \\(N \\times p\\) 的矩陣 \\(X\\)，我們可以將上式改寫為\n\\[\\begin{align*}\n\\min_{V_q} \\ & \\mathrm{tr}(X(I-V_qV_q^T)(I-V_qV_q^T))X^T) \\\\\n=& \\ \\mathrm{tr}((I-V_qV_q^T)X^TX(I-V_qV_q^T)) \\\\\n=& \\ \\mathrm{tr}(X^TX - V_qV_q^TX^TX - V_qV_q^TX^TX + V_qV_q^TX^TXV_qV_q^T) \\\\\n=& \\ \\mathrm{tr}(X^TX) - \\mathrm{tr}(V_qV_q^TX^TX)\n\\end{align*}\\]\n也就是說，我們需要最大化 \\(\\mathrm{tr}(V_qV_q^TX^TX) = \\mathrm{tr}(V_q^TX^TXV_q) = \\sum_{i=1}^qv_i^TX^TXv_i\\)。\n對 \\(X\\) 進行 SVD：\\(X = UDV^T\\)，所以 \\(X^TX=VD^2V^T\\)。則上式成為\n\\[\n\\sum_{i=1}^qv_i^TVD^2V^Tv_i=\\sum_{i=1}^qv_i^T(\\sum_{j=1}^pdi^2\\tilde{v}_i\\tilde{v}_i^T)v_i= \\sum_{i=1}^q\\sum_{j=1}^pdi^2(vi^T\\tilde{v}_i)^2\n\\]\n其中 \\(d_i\\) 為 \\(X\\) 的 singular value 且 \\(d_1 \\geq d_2 \\geq \\cdots \\geq d_p\\)。為了得到加總的最大值，我們取 \\(v_i=\\tilde{v}_i, \\ i=1,\\cdots,q\\)。\n結論：要找最佳的近似基底\\(V_q\\)，我們對 \\(X\\) 做 SVD，並取前 q 個 right singular vectors.\n\\(\\mathrm{\\square}\\)"
  },
  {
    "objectID": "Ch2/Sec2.3.html#機器學習的種類",
    "href": "Ch2/Sec2.3.html#機器學習的種類",
    "title": "統計學習理論",
    "section": "機器學習的種類",
    "text": "機器學習的種類\n在機器學習的問題中，我們希望能透過適當的演算方法幫助我們進行客觀或自動的判斷，機器學習的方法可以分為兩大類：「監督式學習」與「非監督式學習」。\n\n監督式學習：監督式學習的資料可以想成是一組 \\((\\bf{X},y)\\) 的模式，其中包含我們搜集到的資料 \\(\\bf{X}\\) ，以及我們想要預測的結果 \\(y\\) 。而 \\(y\\) 也可以類別資料，此即所稱的分類問題(classification problems)。\n非監督式學習：非監督式學習的資料只有 \\(\\bf{X}\\) ，我們希望透過這些資料來找出一個脈絡，如分群問題(clustering problems)。\n\n換句話說，監督式學習的分析方法適於於存在「正確解答」的模式，我們可以透過建立「訓練集(training set)」以及「測試集(test set)」來比對訓練的優劣。"
  },
  {
    "objectID": "Ch2/Sec2.3.html#基本的預測方法",
    "href": "Ch2/Sec2.3.html#基本的預測方法",
    "title": "統計學習理論",
    "section": "基本的預測方法",
    "text": "基本的預測方法\n在 The Elements of Statistical Learning 中的 2.3 節介紹了兩種預測的方法，分別為：「線性迴歸預測」以及「KNN 演算法」。\n\n資料集\n在這個資料集的預測變數 \\(\\bf{X}\\) 有兩個變數，而被預測變數 \\(Y\\) 有兩個分別為橘色與藍色。\n我們定義： \\[\\hat{G}=\\left\\{ \\begin{matrix}\n{\\color{Orange} orange} & , \\hat{Y}>0.5\\\\\n{\\color{Blue} blue} & , \\hat{Y} \\leq 0.5\n\\end{matrix}\\right.\\]\n並且從課本所附的資料集匯入資料：\n\n%load_ext rpy2.ipython #讓R可以被用\n\n\n%%R # 啟用R\n\n#Import data\nload(url(\"https://hastie.su.domains/ElemStatLearn/datasets/ESL.mixture.rda\"))\n\nx <-  ESL.mixture$x\ng <-  ESL.mixture$y\nxnew <- ESL.mixture$xnew\n#grid\npx1 <- ESL.mixture$px1\npx2 <- ESL.mixture$px2\n\n\n\n最小平方法\n最小平方法希望能找到一條直線(or hyperplane)來分出我們要的資料，可以將這條線寫成： \\[ \\hat{Y}=X\\hat{\\beta} \\]\n因此，我們要找到對應的最佳係數估計 \\(\\hat{\\beta}\\)，其運算邏輯為要極小化殘差平方和(residual sum of squares)：\n\\[ RSS(\\beta)= (y - X \\beta)^T (y - X \\beta)\\]\n我們可以得到以下結果： \\[\\hat{\\beta }= (X^TX)^{-1}X^Ty\\]\n在 R 中，我們可以很簡單地使用lm()函數來進行最小平方法的估計。\n\n%%R\nlinear <- lm(g~x)\nlinear$coefficients\n\n(Intercept)          x1          x2 \n  0.3290614  -0.0226360   0.2495983 \n\n\n\n%%R\n#計算期望機率\nprob_linear <- linear$coefficients[1] +linear$coefficients[2] * xnew[,1] + linear$coefficients[3] * xnew[,2]\n#fit grids\nprob_lm <- matrix(prob_linear, length(px1), length(px2))\n\npar(mar=rep(2,4)) #匡出圖片的範圍\ncontour(px1, px2, prob_lm, levels=0.5, labels=\"\", xlab=\"\", ylab=\"\", main=\n          \"Linear regression of 0/1 response\", axes=FALSE)\npoints(x, col=ifelse(g==1, \"orange\", \"cornflowerblue\"))\ngd <- expand.grid(x=px1, y=px2)\npoints(gd, pch=\".\", cex=1.2, col=ifelse(prob_linear>0.5, \"orange\", \"cornflowerblue\"))\nbox()\n\n\n\n\n\n\nKNN 演算法\nKNN(K Nearest Neighbors)演算法的觀念相當簡單，也就是找出各資料點最近的 \\(k\\) 個資料點(k個鄰居)，並且進行投票，當藉此找出該點究竟是誰。\nKNN的投票邏輯可以這樣表示： \\[  \\hat{Y} (x)= \\frac{1}{k}\\sum_{x_i \\in N_k(x)}^{}y_i  \\]\n\n準備好你的樣本，決定一個整數 \\(k\\)\n決定與各樣本點最接近的 \\(k\\) 個樣本點，距離可以有以下選擇：\n\n\n歐式距離(Euclidean distance)：\\(d(x,x_i)=\\sqrt{(x-x_i)^T(x-x_i)}\\)\n曼哈頓距離(Manhattan distance)： \\(d(x,x_i)=\\sum_1^j|x_j-x_{ij}|\\)\n明氏距離(Minkowski distance)：\\(d(x,x_i)=(\\sum_{s=1}^j|x_j-x_{ij}|^p)^{\\frac{1}{p}}\\)\n\n\n開始投票！看鄰居是哪種類的資料最多，則該資料點 \\(x\\) 就會被分到該類別。\n\nKNN 演算法也要注意儘量選擇奇數的\\(k\\)，藉此避免平手，而最適合的\\(k\\)可透過訓練集測試集的比對來找尋，也就是測試 k=1, k=2, …。在高維度的資料中，KNN的表現也可能不好。\n在 R 語言，我們需要使用class套件來呼叫knn()函式，即可進行KNN演算法的運算。\n\n%%R\n#KNN function\nlibrary(class) #Knn\n#knn(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)\nmod15 <- knn(x, xnew, g, k=15, prob=TRUE)\n\n\n%%R\nprob <- attr(mod15, \"prob\")\nprob <- ifelse(mod15==\"1\", prob, 1-prob)\npx1 <- ESL.mixture$px1\npx2 <- ESL.mixture$px2\nprob15 <- matrix(prob, length(px1), length(px2))\npar(mar=rep(2,4))\ncontour(px1, px2, prob15, levels=0.5, labels=\"\", xlab=\"\", ylab=\"\", main=\n          \"15-nearest neighbour\", axes=FALSE)\npoints(x, col=ifelse(g==1, \"orange\", \"cornflowerblue\"))\ngd <- expand.grid(x=px1, y=px2)\npoints(gd, pch=\".\", cex=1.2, col=ifelse(prob15>0.5, \"orange\", \"cornflowerblue\"))\nbox()\n\n\n\n\n而我們也可來看看一個極端的例子，也就是當 \\(k=1\\) 時，模仿上面的方法，我們可得到下圖：\n\n%%R\n#1nn\nmod1 <- knn(x, xnew, g, k=1, prob=TRUE)\nprob <- attr(mod1, \"prob\")\nprob <- ifelse(mod1==\"1\", prob, 1-prob)\npx1 <- ESL.mixture$px1\npx2 <- ESL.mixture$px2\nprob1 <- matrix(prob, length(px1), length(px2))\npar(mar=rep(2,4))\ncontour(px1, px2, prob1, levels=0.5, labels=\"\", xlab=\"\", ylab=\"\", main=\n          \"1-nearest neighbour\", axes=FALSE)\npoints(x, col=ifelse(g==1, \"orange\", \"cornflowerblue\"))\ngd <- expand.grid(x=px1, y=px2)\npoints(gd, pch=\".\", cex=1.2, col=ifelse(prob15>0.5, \"orange\", \"cornflowerblue\"))\nbox()\n\n\n\n\n我們可以統整 KNN 演算法為有 \\(N/k\\) 個參數(parameters)，可以想成此分群共有 \\(N/k\\) 個區域也就是如同要決定\\(N/k\\)中心點一般。\n\n\nBayes Classifier\nhttps://esl.hohoweiya.xyz/02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory/index.html\nhttps://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf\nhttps://blog.csdn.net/zejianli/article/details/53857581"
  },
  {
    "objectID": "Ch4/Sec4.0.html",
    "href": "Ch4/Sec4.0.html",
    "title": "Overview",
    "section": "",
    "text": "在這一個章節，作者從簡單的線性模型開始，介紹了三個建立分類模型的方式。\n第一個方法先得到聯合機率分配\\(P(X, G)\\)，再透過貝氏定理得到資料點的後驗機率 \\(Pr(G=k|X=x)\\)。透過這種方式建立的模型為生成模型(generative model)，因為我們知道資料是從哪個分配抽出的。而在常態分配的假設下，我們可以得到線性的生成模型：即本章所要介紹的 Linear Discriminant Analysis (LDA) 和 Quadratic Discriminant Analysis (QDA)。\n4.3 Linear Discriminant Analysis(LDA)\n第二個方式偏向頻率學派的思維，直接擬和後驗機率 \\(Pr(G=k|X=x)\\)，過程中透過 最大化 (conditional) log-likelihood 來認定模型參數。這樣的分類模型我們稱為判別模型 (discriminative model)。本章要介紹的 Logistic Regression 即屬於此類。\n4.4 Logistic Regression\n第三個方式不依賴於統計理論，而是直接找出一個超平面 (hyperplane) 把手邊的訓練資料的類別區分開來，就例如本章提到的 Perceptron。而如果資料可以被完全正確分類，將有很多個超平面都可以達到 100% 的正確率。此時我們希望找出一個「最好」的超平面 (Optimal Seperating Hyperplane)，和各類別之間最大地保留間隔，這也是後面 SVM 的線性原型。"
  },
  {
    "objectID": "Ch4/Sec4.2.html",
    "href": "Ch4/Sec4.2.html",
    "title": "統計學習理論",
    "section": "",
    "text": "4.2 Linear Regression of an Indicator Matrix\n若我們說， \\(\\mathcal{G}\\) 有 \\(K\\) 個類別，為了表示該筆資料是何種類別，我們可以建立一個 \\(Y\\) 矩陣, \\(Y=(Y_1,...,Y_k)_{N \\times K}\\)，也就是說 \\(Y\\) 矩陣中有 \\(N\\) 個 \\(K\\) 維的 row vectors。而根據線性迴歸模型，我們可得對 \\(Y\\) 的估計為：\n\\[ \\hat{Y}  = X(X^TX)^{-1}X^TY=X \\hat{B} \\]\n\\(B_{(p+1)\\times K}\\)： \\(p\\) 個 inputs 然後加上截距項。\n或者是從另外一個觀點，也就是我們希望極小化 \\(\\hat{y}\\) 跟 \\(y\\) 的距離。\n\\[\\min_{\\bf{B}}\\sum_{i=1}^N||y_i-[(1,x_i^T) \\textbf{B} ]^T ||^2\\]\n也就是說， \\(\\hat{f}(x)\\) 會分類到最接近的目標群( \\(y_i=t_k,\\ if\\ g_i = k\\) )：\n\\[\\hat{G}(x)=\\mathop{\\arg\\min}\\limits_{k}||\\hat{f}(x)-t_k||^2\\]\n#模擬Fig. 4.3 https://esl.hohoweiya.xyz/notes/LDA/sim-4-3/index.html"
  },
  {
    "objectID": "Ch4/Sec4.3.html#linear-discriminant-analysislda",
    "href": "Ch4/Sec4.3.html#linear-discriminant-analysislda",
    "title": "統計學習理論",
    "section": "4.3 Linear Discriminant Analysis(LDA)",
    "text": "4.3 Linear Discriminant Analysis(LDA)\n在課本的2.4節，我們知道在做分類決策時，我們是在極大化某種後驗機率(posrerior probability)，也就給定 \\(X=x\\) 時，找一個最大可能性的分類當作分析的結果。 也就是說： 我們令分類所做的預測 \\(\\hat{G}(x)=\\mathcal{G}_k\\) ( \\(\\mathcal{G}_k\\) 為其中一種分類)； 也就表示，當給定 \\(X=x\\) 的條件機率之下， \\(\\mathcal{G}_k\\) 是最有可能的出象，換成數學的語言就是： \\(P(\\mathcal{G}_k|X=x)=\\max_{l}P(G=\\mathcal{G}_l|X=x)\\)\n（或者說是 \\(=\\max_{l}P(G=l|X=x)\\) ）\n有這個基本觀念後，我們設定資料屬於類別 \\(k\\) 的先驗機率為： \\(\\pi_k=P(G=k)\\) ，而當然 \\(\\sum_{k=1}^K\\pi_k =1\\)。\n而透過貝氏定理，我們可以得到以下關係：\n\\[P(G=k|X=x)=\\frac{{\\color{Red} f_k(x)}{\\color{Blue} \\pi_k}}{{\\color{DarkOrange} \\Sigma_{l=1}^k f_l(x)\\pi_l}}=\\frac{{\\color{Red} P(X|G=k)}{\\color{Blue} P(G=k)} }{{\\color{DarkOrange} \\Sigma_l P(X|G=l)P(G=l)}}\\]\n根據上式，我們必須對 \\(f_k(x)\\) 做一些假設，這也就是當資料是第 \\(k\\) 類時， \\(X\\) 的機率密度函數。我們設定各分類的機率密度服從「多變量常態分配(Multivariate Normal Distribution)」。\n\\[f_k(x)=\\frac{1}{(2\\pi)^{\\frac{p}{2}}|\\Sigma_k|^\\frac{1}{2}}e^{-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)}\\]\n要注意的是，在LDA的架構之下，所有類別的pdf均享有相同的共變數矩陣，也就是 \\(\\Sigma_k=\\Sigma,\\forall k\\)。\n接著，我們就可以以去比較兩兩類別之間的後驗發生機率 \\(P(G|X)\\) ，我們在此利用對數的良好性質來分析兩者關係，假設我們現在要探討類別 \\(k\\) 和類別 \\(l\\) ，誰的發生機率大呢？我們用下列關係式來表達：\n\\[\\begin{aligned}\n\\ln\\frac{P(G=k|X=x)}{P(G=l|X=x)}&=\\ln \\frac{f_k(x)\\pi_k}{f_l(x)\\pi_l}=\\ln\\frac{\\pi_k}{\\pi_l}+\\ln {f_k(x)}-\\ln{f_l(x)}\\\\\n&=\\ln\\frac{\\pi_k}{\\pi_l}-\\frac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)+\\frac{1}{2}(x-\\mu_l)^T\\Sigma^{-1}(x-\\mu_l)\\\\\n&=\\ln\\frac{\\pi_k}{\\pi_l}-\\frac{1}{2}(\\mu_k+\\mu_l)^T\\Sigma^{-1}(\\mu_k-\\mu_l)+x^T\\Sigma^{-1}(\\mu_k-\\mu_l)\n\\end{aligned}\\]\n注意!這樣良好的線性性質是來自於我們假設兩個分類具有相同的共變異數矩陣。若我們對類別 \\(k\\) 和類別 \\(l\\) 的分界線感興趣，其分界線就是位在兩者機率密度相等之處，也就是當上式等於0時。\n透過相同的想法，我們可以建立一個線性判別函數(linear discriminant function) \\(\\delta_k(x)\\) ，來決定該資料應被分配到哪一個類別，也就是 \\(G(x)=\\mathop{\\arg\\max}\\limits_{k}\\delta_k(x)\\) 。 線性判別函數如下所示：\n\\[\\delta_k(x)=x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k +\\ln \\pi_k\\]\n線性判別函數的推導來自以下的成比例關係：\n\\[\\begin{aligned}\nP(G=k|X=x)&\\propto f_k(x)\\pi_k \\\\\n&\\propto -\\frac{1}{2}(x-\\mu_k)^T \\Sigma^{-1} (x-\\mu_k)+\\ln \\pi_k = -\\frac{1}{2}x^T\\Sigma^{-1}x+x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k +\\ln \\pi_k\\\\\n&\\propto  x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k +\\ln \\pi_k \\equiv  \\delta_k(x)\n\\end{aligned}\\]\n正如古典的統計分析一樣，我們並無法知道母體分配的參數，故我們在進行統計學習時，則選擇使用訓練集的資料來估計母體參數。\n\\[\\begin{aligned}\n&\\hat{\\pi_k}=N_k/N \\\\\n&\\hat{\\mu_k}=\\sum_{g_i=k}x_i/N_k \\\\\n&\\hat{\\Sigma}=\\sum_{k=1}^K\\sum_{g_i=k}(x_i-\\hat{\\mu_k})(x_i-\\hat{\\mu_k})^T/(N-K)\n\\end{aligned}\\]\n其中 \\(N_k\\) 是類別\\(k\\)在訓練集中的數量(observations)。\n\n\n2-class LDA (from Ex. 4.2)\nSuppose we have features \\(x \\in \\mathbb{R}^p\\), a two-class response, with class sizes \\(N_1\\), \\(N_2\\), and the target coded as \\(−N/N_1\\), \\(N/N_2\\).\n(a) Show that the LDA rule classifies to class 2 if \\[x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1}) > \\frac{1}{2}(\\hat{\\mu_2}+\\hat{\\mu_1})^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1})-\\ln(N_2/N_1)\\] and class 1 otherwise.\nSol:\n在二元的分類中，我們可以回溯到先前講到的log-odds的觀念，也就是建立此式來做比較(改寫自課本式4.9)：\n\\[\\begin{aligned}\n\\ln\\frac{P(G=2|X=x)}{P(G=1|X=x)}\n=\\ln\\frac{\\pi_2}{\\pi_1}-\\frac{1}{2}(\\mu_2+\\mu_1)^T\\Sigma^{-1}(\\mu_2-\\mu_1)+x^T\\Sigma^{-1}(\\mu_2-\\mu_1)\n\\end{aligned}\\]\n根據對數性質，若此式大於0，就表示 \\(P(G=2|X=x)\\) 的機率相對於 \\(P(G=1|X=x)\\) 來得高，故我們自然會將其分類到 class 2。 而我們根據訓練集對上式做估計並且做些許代數運算即可得到決策的函數： \\[x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1}) > \\frac{1}{2}(\\hat{\\mu_2}+\\hat{\\mu_1})\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1})-\\ln(N_2/N_1)\\]\n(b) Consider minimization of the least squares criterion \\[\\sum^N_{i=1}(y_i-\\beta_0-x_i^T\\beta)^2\\]\nShow that the solution \\(\\hat{\\beta}\\) satisfies\n\\[[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}_B]\\beta=N(\\hat{\\mu_2}-\\hat{\\mu_1})\\]\nwhere, \\[\\hat{\\Sigma}_B=\\frac{N_1N_2}{N^2}(\\hat{\\mu_2}-\\hat{\\mu_1})(\\hat{\\mu_2}-\\hat{\\mu_1})^T\\]\nSol:\n首先，我們知道 \\(N=N_1+N_2\\)。 而這個線性迴歸方程包含常數項，則根據 Normal equation 可以得到對 \\(\\hat{\\beta_0}\\) 和 \\(\\hat{\\beta_1}\\) 的估計：\n\\[ \\begin{bmatrix}\n\\hat{\\beta_0}\\\\\n\\hat{\\beta}\n\\end{bmatrix}=(X^TX)^{-1}X^Ty \\]\n其中 designed matrix 可這樣表示：( \\(X\\) 裡面有 \\(\\bf{1}\\) 向量)\n\\[X^TX=\\begin{bmatrix}\nN & \\sum_{i=1}^N x_i^T\\\\\n\\sum_{i=1}^N x_i & \\sum_{i=1}^Nx_ix_i^T\n\\end{bmatrix}\\]\n其中\n\\[\\sum_{i=1}^Nx_i =\\sum_{i=1}^{N_1}x_i+\\sum_{i=1}^{N_2}x_i = N_1\\hat{\\mu_1}+ N_2\\hat{\\mu_2}\\]\n又知\n\\[\\begin{aligned}\n\\hat{\\Sigma} &= \\frac{1}{N-2}\\sum_{k=1}^2\\sum_{g_i=k}(x_i-\\hat{\\mu_k})(x_i-\\hat{\\mu_k})^T \\\\\n&=\\frac{1}{N-2}[\\sum_{g_i=1}x_ix_i^T-N_1 \\mu_1 \\mu_1^T+\\sum_{g_i=2}x_ix_i^T-N_2\\mu_2 \\mu_2^T]\n\\end{aligned}\\]\n因此\n\\[\\sum_{i=1}^Nx_ix_i^T = (N-2)\\hat{\\Sigma}+N_1 \\mu_1 \\mu_1^T+N_2\\mu_2 \\mu_2^T\\]\n若我們將屬於類別1的資料放在矩陣的前 \\(N_1\\) 個，而類別2的資料 \\(N_2\\) 個緊接在後，\\(X^Ty\\) 可以寫成：\n\\[ X^Ty= \\begin{bmatrix}\n1 &\\cdots  & 1 &1  & \\cdots &1 \\\\\nx_1 &\\cdots  &x_{N_1}  &x_{N_1+1}  &\\cdots  & x_{N_1+N_2}\n\\end{bmatrix}\n\\begin{bmatrix}\n-N/N_1\\\\\n\\vdots\\\\\n-N/N_1\\\\\nN/N_2\\\\\n\\vdots\\\\\nN/N_2\n\\end{bmatrix} =\\begin{bmatrix}\n0 \\\\\n-N\\mu_1+N\\mu_2\n\\end{bmatrix}\\]\n整理先前計算出的這些東西代入 Normal equation，整理後可得：\n(註： \\(\\beta_0=(-\\frac{N_1}{N}\\mu_1^T-\\frac{N_2}{N}\\mu_2^T)\\beta\\) )\n\\[(N_1\\mu_1+N_2\\mu_2)(-\\frac{N_1}{N}\\mu_1^T-\\frac{N_2}{N}\\mu_2^T)\\beta+((N-2)\\hat{\\Sigma} +N_1\\mu_1\\mu_1^T+N_2\\mu_2\\mu_2^T)\\beta=N(\\mu_2-\\mu_1)\\]\n經過一些運算，並引入題目對 \\(\\hat{\\Sigma_B}\\) 的定義，即可得：\n\\[[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}_B]\\beta=N(\\hat{\\mu_2}-\\hat{\\mu_1})\\]\n(c) Hence show that \\(\\hat{\\Sigma}_B \\beta\\) is in the direction \\((\\hat{\\mu_2} - \\hat{\\mu_1})\\) and thus\n\\[\\hat{\\beta}\\propto \\hat{\\Sigma}^{-1} (\\hat{\\mu_2} - \\hat{\\mu_1})\\]\nTherefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.\nSol:\n若我們直接計算 \\(\\hat{\\Sigma}_B \\beta\\) ，由於 \\((\\hat{\\mu_2}-\\hat{\\mu_1})^T \\beta\\) 內積後是一個scalar。也就是\n\\[\\hat{\\Sigma}_B \\beta=\\frac{N_1N_2}{N^2}(\\hat{\\mu_2}-\\hat{\\mu_1}){\\color{Red} (\\hat{\\mu_2}-\\hat{\\mu_1})^T \\beta}=\\frac{N_1N_2}{N^2}(\\hat{\\mu_2}-\\hat{\\mu_1}){\\color{Red} c}\\]\n換句話說， \\(\\hat{\\Sigma}_B \\beta\\) 和 \\((\\hat{\\mu_2}-\\hat{\\mu_1})\\) 差了 \\({ \\frac{N_1N_2}{N^2}c}\\) 倍。又這些常數都為正，故得知： \\[ \\hat{\\beta} \\propto \\hat{\\Sigma}^{-1} (\\hat{\\mu_2}-\\hat{\\mu_1}) \\]\n此即表示線性迴歸與LDA其實是有相像的運算邏輯。\n(d) Show that this result holds for any (distinct) coding of the two classes.\nSol:\n在上一題中，我們並沒有特別對 \\(t_k\\) 有特別假設，這樣的性質是來自於一開始對 \\(\\hat{\\Sigma}_B\\) 的設計，並且與 \\(\\beta\\) 內積後出現純數而推演出這樣的比例關係。\n(e) Find the solution \\(\\hat{\\beta_0}\\) (up to the same scalar multiple as in (c), and hence the predicted value \\(\\hat{f}(x)=\\hat{\\beta_0}+x^T\\hat{\\beta}\\). Consider the following rule: classify to class 2 if \\(f(x) > 0\\)and class 1 otherwise. Show this is not the same as the LDA rule unless the classes have equal numbers of observations.\nSol:\n在先前的討論，我們知道：\n\\[\\beta_0 = -\\frac{1}{N}(N_1\\hat{\\mu}^T_1+N_2\\hat{\\mu}^T_2)\\hat{\\beta}\\]\n我們可改寫 \\(\\hat{f}(x)\\) 如下：\n\\[\\begin{aligned}\n\\hat{f}(x)&=-\\frac{1}{N}(N_1\\hat{\\mu}^T_1+N_2\\hat{\\mu}^T_2-Nx^T)    \\hat{\\beta }\\\\\n&=-\\frac{1}{N}(N_1\\hat{\\mu}^T_1+N_2\\hat{\\mu}^T_2-Nx^T) {\\color{Red} \\lambda \\hat{\\Sigma}^{-1}(\\hat{\\mu_2}-\\hat{\\mu_1})}\n\\end{aligned}\\]\n經過展開，並重整，則可得到下式：\n\\[x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1}) > \\frac{1}{N}(N_2\\hat{\\mu_2}+N_1\\hat{\\mu_1})^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1})\\] 若 \\(N_1=N_2\\) ，則 \\(N_1/N=N_2/N=1/2\\)，而 \\(\\ln(N_2/N_1)=\\ln 1=0\\)\n這樣的決策準則就跟LDA一樣了！"
  },
  {
    "objectID": "Ch4/Sec4.3.html#quadratic-discriminant-analysis-qda",
    "href": "Ch4/Sec4.3.html#quadratic-discriminant-analysis-qda",
    "title": "統計學習理論",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\n還記得先前在推導 LDA 時除了常態假設之外，還有一個變異數矩陣相等的假設，若我們放寬這個假設，這個方法就變成 QDA ，Quadratic 的部分就是來自於多變量常態分配中在 \\(e\\) 上的 \\(x\\) 二次式不會被對消。\n二次判別函數(Quadratic Discriminant function)如下：\n\\[\\delta_k(x)=-\\frac{1}{2}\\ln|\\Sigma_k|-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)+\\ln \\pi_k\\]\n\\(l\\) 個類別的分界線可以這樣表示\n\\[\\{x|\\delta_k(x)=\\delta_l(x) \\}\\]\nQDA 有參數太多的缺點。"
  },
  {
    "objectID": "Ch4/Sec4.3.html#regulized-discriminant-analysis-rda",
    "href": "Ch4/Sec4.3.html#regulized-discriminant-analysis-rda",
    "title": "統計學習理論",
    "section": "Regulized Discriminant Analysis (RDA)",
    "text": "Regulized Discriminant Analysis (RDA)\n\nFriedman(1989) 提出了介於LDA與QDA之間的方法，稱之為 Regulized Discriminant Analysis (RDA)。正則化的共變異數矩陣如下所示：\n\\[\\hat{\\Sigma}_k(\\alpha)=\\alpha \\hat{\\Sigma}_k+(1-\\alpha)\\hat{\\Sigma},\\ \\alpha \\in[0,1]\\]\n在此， \\(\\hat{\\Sigma}\\) 是LDA所做的共變數矩陣，而 \\(\\hat{\\Sigma}_k\\) 為QDA的共變數矩陣。\nby 課本\n\\[\\hat{\\Sigma}(\\gamma)=\\gamma\\hat{\\Sigma}+(1-\\gamma)\\hat{\\sigma}^2I\\]"
  },
  {
    "objectID": "Ch4/Sec4.3.html#lda-的計算",
    "href": "Ch4/Sec4.3.html#lda-的計算",
    "title": "統計學習理論",
    "section": "LDA 的計算",
    "text": "LDA 的計算"
  },
  {
    "objectID": "Ch4/Sec4.3.html#reduced-rank-lda",
    "href": "Ch4/Sec4.3.html#reduced-rank-lda",
    "title": "統計學習理論",
    "section": "Reduced-Rank LDA",
    "text": "Reduced-Rank LDA"
  },
  {
    "objectID": "Ch4/Sec4.3.html#lda-和-linear-regression-的關係-from-ex.-4.3",
    "href": "Ch4/Sec4.3.html#lda-和-linear-regression-的關係-from-ex.-4.3",
    "title": "統計學習理論",
    "section": "LDA 和 linear regression 的關係 (from Ex. 4.3)",
    "text": "LDA 和 linear regression 的關係 (from Ex. 4.3)\nSuppose that we transform the original predictors \\(\\mathbf{X}\\) to \\(\\mathbf{\\hat{Y}}\\) by taking the predicted values under linear regression. Show that LDA using \\(\\bf{\\hat{Y}}\\) is identical to using LDA in the original space.\nSol: 根據題意，我們知道 \\(x \\in \\mathbb{R}^p\\) 和 \\(y \\in \\mathbb{R}^k\\) ，且：\n\\[\\begin{aligned}\n& \\hat{y}=\\hat{\\mathbf{B}}x \\\\\n& \\mathbf{\\hat{Y}} = \\mathbf{X} \\hat{\\mathbf{B}}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\end{aligned}\\]\n對於任意的類別 \\(k\\) ，我們可以建構出該類別的平均數，我們定義類別 \\(k\\) 對應其 \\(X\\) 以及 \\(Y\\) 的平均數為：\n\\[\\begin{aligned}\n& \\mu_k = \\frac{1}{N_k}\\sum_{g_i=k}x^T_i \\\\\n& \\hat{\\mu_k} = \\mathbf{B}^T\\mu_k\n\\end{aligned}\\]\n若我們定義 \\(\\mathbf{X}\\) 的共變數矩陣為 \\({\\Sigma}\\) ，則 \\(\\hat{\\mathbf{Y}}\\) 的共變數矩陣則為：\n\\[\\hat{\\Sigma} = \\mathbf{B}^T \\Sigma \\mathbf{B}\\]\n其中\n\\[\\begin{aligned}\n& \\Sigma =\\frac{1}{N-K}X^T(I-YD^{-1}Y^T)X\\\\\n&,where\\  D =\\begin{bmatrix}\nn_1 & 0  & \\cdots &0 \\\\\n0 & n_2 &\\cdots  &0 \\\\\n\\vdots&\\vdots  & \\vdots & \\vdots\\\\\n0 &0  & \\cdots & n_k\n\\end{bmatrix}\n\\end{aligned}\\]\n若我們直接用 \\(\\hat{\\mathbf{Y}}\\) 的資料做 LDA，則判別函數為：\n\\[\\delta_k(\\hat{Y})= \\hat{\\mathbf{Y}}\\hat{\\Sigma}^{-1}\\hat{\\mu_k}-\\frac{1}{2}\\hat{\\mu_k}^T\\hat{\\Sigma}^{-1}\\hat{\\mu_k}+\\ln \\pi_k\\]\n第一項可以改寫如下(全部寫成矩陣的樣子)：( \\(B=(X^TX)^{-1}X^TY\\) )\n\\[\\begin{aligned}\n\\hat{\\mathbf{Y}}\\hat{\\Sigma}^{-1}\\hat{\\mu}&=(XB)(B^T\\Sigma B)^{-1}(B^TX^TYD^{-1}) \\\\\n&=X\\hat{\\Sigma}^{-1}\\mu\n\\end{aligned}\\]\n這就是使用 \\(\\mathbf{X}\\) 進行LDA的其中一部分。\n而第二項，若將全部 \\(K\\) 種類的平均搜集起來，寫成矩陣 \\(\\hat{\\mu}\\) 可改寫為：\n\\[\\begin{aligned}\n\\hat{\\mu_k}^T\\hat{\\Sigma}^{-1}\\hat{\\mu}&=(B^T \\mu_k)^T\\hat{\\Sigma}^{-1}(B^TX^TYD^{-1})\\\\\n&=\\mu_k^TB\\hat{\\Sigma}^{-1}(B^TX^TYD^{-1})\\\\\n&=\\mu_k^TB(B^T\\Sigma B)^{-1}B^TX^TYD^{-1}\\\\\n&=\\mu_k^T\\Sigma^{-1}\\mu\n\\end{aligned}\\]\n此即使用 \\(\\mathbf{X}\\) 做LDA的另外一部分。 所以我們可以知道，若我們透過迴歸方程的模式找出 \\(\\hat{\\mathbf{Y}}\\) ，我們便可從對 \\(\\hat{\\mathbf{Y}}\\) 做LDA轉換成對 \\(\\mathbf{X}\\) 做LDA。"
  },
  {
    "objectID": "Ch4/Sec4.4.html",
    "href": "Ch4/Sec4.4.html",
    "title": "統計學習理論",
    "section": "",
    "text": "4.4 Logistic Regression\n羅吉斯迴歸(Logistic Regression)是針對 \\(K\\) 個類別的後驗機率做一個線性的的模型，而跟一般線性迴歸不同的地方是在於，羅吉斯迴歸透過對數的轉換將機率的取值保留在\\([0,1]\\)之間。若我們的資料有 \\(K\\) 類別，則羅吉斯回歸透過建立 \\(K-1\\) 個 log-odds 來建構我們所要的模型，少的那一個類別就拿來作為對照組之用。 以 \\(K\\) 作為基準，Logistic Regression 會建立這些模型關係：\n\\[\\begin{aligned}\n\\ln \\frac{P(G=1|X=x)}{P(G=K|X=x)} &= \\beta_{10}+\\beta_1^Tx\\\\\n\\ln \\frac{P(G=2|X=x)}{P(G=K|X=x)} &= \\beta_{20}+\\beta_2^Tx \\\\\n& \\vdots \\\\\n\\ln \\frac{P(G=K-1|X=x)}{P(G=K|X=x)} &= \\beta_{(K-1)0}+\\beta_{K-1}^Tx\n\\end{aligned}\\]\n我們可以對這些式子取指數回來，可得：\n\\[P(G=l|X=x) =P(G=K|X=x) e^{\\beta_{l0}+\\beta_l^Tx}, l=1,2,\\cdots,N-1\\]\n又\n\\[\\begin{aligned}\nP(G=K|X=x) &= 1-\\sum_{l=1}^{K-1}P(G=l|X=x)\\\\&=1-P(G=K|X=x)\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_l^Tx}\n\\end{aligned} \\]\n故可求解出：\n\\[P(G=K|X=x) = \\frac{1}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_l^Tx} } \\] 以及,\n\\[P(G=k|X=x) = \\frac{e^{\\beta_{k0}+\\beta_k^Tx} }{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_l^Tx} }\\]\n若我們簡化表示，將這些參數定義為：\n\\[\\theta = \\{\\beta_{10},\\beta_1^T,\\cdots,\\beta_{(K-1)0},\\beta_{(K-1)}^T  \\}\\] 則類別\\(k\\)機率可以表示成： \\(P(G=k|X=x)=p_k(x;\\theta)\\)\n\n配適 Logistic Regression 的參數\n在共有 \\(K\\) 類的資料中，可以把資料想成是從多項分配(multinomial)取出來的。因此 likelihood 可以想成是：\n\\[L = \\prod _{i=1}^Np_{g_i}(x_i)\\]\n(只有其中一項的指數會是 \\(1\\) ，其他都是 \\(0\\) ) 將之轉為 log-likelihood：\n\\[l=\\sum _{i=1}^N \\ln p_{g_i}(x_i)\\]\n若我們只先看 \\(K=2\\) 的情形，則：(可參考課本4.4.1)\n\\[l(\\beta) = \\sum_{i=1}^N\\{y_i\\beta^Tx_i -\\ln(1+e^{\\beta^Tx_i})  \\}\\]\n我們求解 log-likelihood 的極值，另一階導數為0：\n\\[\\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^N x_i(y_i-p(x_i;\\beta))=0\\]\n其中 \\(p(x_i;\\beta) = \\frac{e^{\\beta^Tx}}{1+e^{\\beta^Tx}}\\) 。\n我們將一階條件，擴寫成矩陣形式：\n\\[\\frac{\\partial l(\\beta)}{\\partial \\beta} = \\mathbf{X}^T(y-p)\\]\n二階條件的 Hessian matrix 如下：\n\\[\\frac{\\partial^2 l(\\beta)}{\\partial \\beta\\beta^T} = -\\mathbf{X}^T\\mathbf{WX}\\]\n\n 是第 \\(i\\) 個元素是 \\(p(x_i;\\beta^{old})(1-p(x_i;\\beta^{old}))\\) 的對角矩陣。\n\n由於此問題的一階條件沒有 closed-form，因此我們使用牛頓法找根：\n\\[\\begin{aligned}\n\\beta^{new} &= \\beta^{old} + (\\mathbf{W}^T\\mathbf{WX})^{-1}\\mathbf{X}^T(\\mathbf{y}-\\mathbf{p})\\\\\n&= (\\mathbf{W}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W}(\\mathbf{X}\\beta^{old}+\\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p}))\\\\\n&=(\\mathbf{W}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W}\\mathbf{z}\n\\end{aligned}\\]\n觀察上式，我們令 \\(\\mathbf{z}=\\mathbf{X}\\beta^{old}+\\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p})\\)，則我們每進行一次迭代，都是彷彿在進行一次 weighted least squared，故稱之為 iteratively reweighted least squared(IRLS)。在迭代的過程中，我們可以選擇從 \\(\\beta=0\\) 開始。\n\\[\\beta^{new} \\leftarrow \\mathop{\\arg\\min}_\\beta (\\mathbf{z}-\\mathbf{X}\\beta)^T\\mathbf{W}(\\mathbf{z}-\\mathbf{X}\\beta)\\]\n\n\n一個 \\(K=2\\) 的例子，從 \\(x \\in \\mathbb{R}\\) 到 \\(x \\in \\mathbb{R}^p\\) (from Ex. 4.5)\n\nConsider a two-class logistic regression problem with \\(x \\in \\mathbb{R}\\). Characterize the maximum-likelihood estimates of the slope and intercept parameter if the sample \\(x_i\\) for the two classes are separated by a point \\(x \\in \\mathbb{R}\\). Generalize this result to (a) \\(x \\in \\mathbb{R}^p\\) (see Figure 4.16), and (b) more than two classes.\n在先前的討論中，我們知道當 \\(K=2\\) 時，log-likelihood function 為\n\\[l(\\beta) = \\sum_{i=1}^N\\{y_i\\beta^Tx_i -\\ln(1+e^{\\beta^Tx_i})  \\}\\]\n而若 \\(x \\in \\mathbb{R}\\)(一維)，則\n\\[y_i =\\left\\{\\begin{matrix}\n0,\\ x_i \\leq x_0\\\\\n1,\\ x_i > x_0\n\\end{matrix}\\right.\\]\n我們考慮最基本的一維羅吉斯回歸模型，的對數概似函數：\n\\[l(\\beta) = \\sum_{i=1}^N\\{y_i\\beta^Tx_i -\\ln(1+e^{\\beta^Tx_i})  \\}\\]\n故\n\\[\\begin{aligned}\nl(\\beta) &= \\sum_{i=1}^N\\{y_i(\\beta_0+\\beta_1x_i) -\\ln(1+e^{(\\beta_0+\\beta_1x_i)})  \\} \\\\\n&=\\sum_{y_i=0}[-\\ln(1+e^{(\\beta_0+\\beta_1x_i)})]+\\sum_{y_i=1}[(\\beta_0+\\beta_1x_i)-\\ln(1+e^{(\\beta_0+\\beta_1x_i)})]\n\\end{aligned}\\]\n若我們想要 maximize log-likelihood，很明顯地，當 \\(\\beta \\rightarrow \\infty\\) 時就會發散，這導致我們無法找到一個 closed-form 的解。\n但很直觀地，當\\(x \\in \\mathbb{R}\\)，由於資料只有一維，那我們在 x 軸上找到 \\(x=x_0\\) ，並在此處畫上一條垂直於 x 軸的線，這樣就能夠明確地劃分出兩個分類。\n\n當 \\(x \\in \\mathbb{R}^p\\) 時，該如何處理呢？(p>1)\n\n我們透過一樣的邏輯擴展 \\(l(\\beta)\\) 用矩陣向量表示：\n\\[\\begin{aligned}\nl(\\beta) &= \\sum_{i=1}^N\\{y_i\\beta^Tx_i -\\ln(1+e^{\\beta^Tx_i})  \\} \\\\\n&=\\sum_{y_i=0}[-\\ln(1+e^{\\beta^Tx_i})]+\\sum_{y_i=1}[\\beta^Tx_i-\\ln(1+e^{\\beta^Tx_i})]\\end{aligned}\\]\n同樣地，我們同樣會發現 \\(l(\\beta)\\) 會發散。\n\n當 \\(K>2\\) 時，要如何處理呢？\n\n直觀上來說，當 \\(K>2\\) 則需要超過一個超平面來協助我們分類，我們同樣能寫出這種情況的 log-likelihood function：\n\\[\\begin{aligned}\nl(\\beta) &= \\sum_{i=1}^N [\\sum_{k=1}^{K-1}\\mathbf{1}_{y_i=k} \\beta_k^Tx_i-\\ln(1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}]\\\\\n&=\\sum_{k=1}^{K-1} \\sum_{g_k}[\\beta_k^Tx_i-\\ln(1+\\sum_{l=1}^{K-1}\\beta_l^Tx_i) ]\n+\\sum_{g_k}[-\\ln(1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i})]\n\\end{aligned}\\]\n而這條式子同樣也找不到 的 closed-form。\n\n\nExample: South African Heart Disease\n\n\nLogistic v.s. LDA"
  },
  {
    "objectID": "Ch5/Sec5.1.html#introduction",
    "href": "Ch5/Sec5.1.html#introduction",
    "title": "統計學習理論",
    "section": "Introduction",
    "text": "Introduction\n在先前我們所討論的模型，如線性迴歸、LDA、羅吉斯迴歸等皆是透過樣本資料建立線性關係，而真實的 \\(f(X)\\) 很可能不是線性的，而發展出線性模型除了方便和易解釋之外，也可以將其視為 \\(f(X)\\) 的一階泰勒逼近。在這個章節，我們希望透過配適各種 \\(X\\) 的轉換來建構 \\(f(X)\\) 。\n我們定義第 \\(m\\) 個 \\(X\\) 的轉換為 \\(h_m(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}\\)，而 \\(m = 1,\\cdots,M\\) ，可得：\n\\[f(X) = \\sum_{m=1}^M \\beta_m h_m(X)\\]\n此即 \\(X\\) 的線性基底展開式(linear basis expansion)，當我們決定好 \\(h_m\\) 後，即可進行配適。以下是一些常見 \\(h_m\\) 的範例：\n\n\\(h_m(X) = X_m,\\ m=1,\\cdots,p\\) 即會轉回原本的線性模型。\n\\(h_m(X) = X^2_j\\) 或 \\(h_m(X)=X_j X_k\\) 也就是多項式的逼近(更高階的泰勒逼近)，但這會導致多項式的維度過大。\n\\(h_m(X) = \\log(X_j),\\sqrt{X_j},\\cdots\\) 此為 \\(X\\) 的非線性轉換。\n\\(h_m(X) = I(L_m \\leq X_k \\leq U_m)\\) ，由於 \\(I(\\cdot)\\) 為指示函數，故模型為此為 piecewise constant model 。\n\n多項式函數是使用基底展開 \\(f(X)\\) 的好例子，但其也有模型的缺點，就是在 remote regions (可以想像是左右兩端) 的震盪較大，但可以較好的配適中間的部分。\n下列有三種比較適當的方式可以提供我們選擇 spline 函數的指引\n\nRestriction methods：也就是事先決定好函數的種類，比如我們限制模型具備可加性，也就是將每個 \\(f_j(\\cdot)\\) 切成 \\(M_j\\) 段相加。\n\n\\[f(X) = \\sum_{j=1}^p f_j(X_j) = \\sum_{j=1}^p \\sum_{m=1}^{M_j}\\beta_{jm}h_{jm}(X_j)\\]\n\nSelection methods：只選擇包含對模型顯著的基底函數，比如說一些 stagewise greedy 的方法 CART、MARS 等方法。\nRegularization methods：我們可以使用所有的 basis dictionary ，但是我們對係數做了限制，比如說 ridge regression。"
  },
  {
    "objectID": "Ch5/Sec5.2.html#b-spline",
    "href": "Ch5/Sec5.2.html#b-spline",
    "title": "統計學習理論",
    "section": "B-spline",
    "text": "B-spline\nB-spline\nB-spline 是一個計算上較有效率的配適方法，我們在 knots 上也有其他的設定，我們定義兩個邊界的(boundary) knots \\(\\xi_0 < \\xi_1\\) 和 \\(\\xi_K < \\xi_{K+1}\\) ，並且定義增廣的 knots 序列 \\(\\tau\\) 如下：\n\\[\\begin{aligned}\n& \\tau_1 \\leq \\tau_2 \\leq \\cdots \\leq \\tau_M \\leq \\xi_0 \\\\\n& \\tau_{j+M}=\\xi_j,\\ j=1,\\cdots,K \\\\\n& \\xi_{K+1} \\leq \\xi_{K+M+1} \\leq \\xi_{K+M+2} \\leq \\cdots \\leq \\xi_{K+2M}\n\\end{aligned}\\]\n我們將 \\(B_{i,m}(x)\\) 記為 knots sequence \\(\\tau\\) 的第 \\(i\\) 個 \\(m\\) 階 B-spline basis function 其中 \\(m \\leq M\\) ，我們遞迴地將其計算如下：\n\\[B_{i,1}(x)=\\left\\{\\begin{matrix}\n1 & if\\ \\tau_1 \\leq x <\\tau_{i+1}  \\\\\n0 & o.w\n\\end{matrix}\\right. ,\\ for \\ i=1,\\cdots,K+2M-1\\]\n\\[B_{i,m}(x)= \\frac{x=\\tau_i}{\\tau_{i+m-1}-\\tau_i}B_{i,m-1}(x)+\\frac{\\tau_{i+m}-x}{\\tau_{i+m}-\\tau_{i+1}}B_{i+1,m-1}(x)\\]\n\n    #install.packages('splines2')\n    par(mfrow = c(4, 1), mar = c(0, 0, 2, 0))\n\n    for (d in 0:3)\n    {\n        bs_d = splines2::bSpline(1:100, degree = d, knots = seq(10, 90, 10), intercept = TRUE)\n        matplot(1:100, bs_d, type = ifelse(d == 0, \"s\", \"l\"), lty = 1, ylab = \"spline\", \n                xaxt = 'n', yaxt = 'n', ylim = c(-0.05, 1.05), lwd = 2)\n        title(paste(\"degree =\", d))\n    }"
  },
  {
    "objectID": "Ch5/Sec5.2.html#natural-cubic-splines",
    "href": "Ch5/Sec5.2.html#natural-cubic-splines",
    "title": "統計學習理論",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\n由於我們對多項式的配適會在邊界處較不穩定，natural cubic splines 要求二階和三階微分在邊界處為0( i.e. \\(min(x)\\) and \\(max(x)\\) )，因此，配適好的模型會在兩個邊界之外(i.e \\((−\\infty, \\xi_1]\\) and \\([\\xi_K, \\infty)\\) )為線性。由於有兩個邊界與兩個限制式，我們因此可以減少四個自由度，而 \\(K\\) 個 knots 的 natural cubic splines 具有 \\(K\\) 個 basis functions。從先前提到的 truncated power series basis 出發，可得：( 課本式 5.4 和 式 5.5 )\n\\[N_1(X)=1,\\ N_2(X)=X,\\ N_{k+2}(X)= d_k(X)-d_{K-1}(X)\\]\n其中，\n\\[d_k(X) = \\frac{(X-\\xi_k)^3_+ - (X-\\xi_K)^3_+ }{\\xi_K-\\xi_k}\\]\n我們可以證明如下：\n\n\nProof of (5.4) and (5.5). (from Ex. 5.4)\nConsider the truncated power series representation for cubic splines with K interior knots. Let\n\\[f(X) = \\sum_{j=0}^3\\beta_j X^j + \\sum_{k=1}^K \\theta_k(X-\\theta_k)^3_+\\]\nProve that the natural boundary conditions for natural cubic splines (Section 5.2.1) imply the following linear constraints on the coefficients:\n\\[\\begin{matrix}\n\\beta_2=0, &  \\sum_{k=1}^K \\theta_k =0  \\\\\n\\beta_3=0, & \\sum_{k=1}^K \\xi_k\\theta_k =0\n\\end{matrix}\\]\nHence derive the basis (5.4) and (5.5).\n我們先看到最左側的區域 \\((-\\infty,\\xi_1)\\) ：\n\\[\\begin{aligned}\nf(x) &= \\sum_{j=0}^3 \\beta_j x^j = \\beta_0 + \\beta_1 x +\\beta_2 x^2+\\beta_3 x^3\\\\\nf'(x) &= \\beta_1 + 2 \\beta_2 x + 3\\beta_3x^3\\\\\nf''(x)&=2\\beta_2+6\\beta_3x\n\\end{aligned}\\]\n而根據 natural cubic splines 對邊界條件的定義，可得： \\(f''(x)=0\\) 且要對 \\(\\forall x \\in (-\\infty,\\xi_1)\\) 都成立，因此 \\(\\beta_2=\\beta_3=0\\) 。\n而我們再看到另外一邊的右邊界 \\((\\xi_k,\\infty)\\) ：\n\\[\\begin{aligned}\nf(x) &= \\sum_{j=0}^3\\beta_j x^j + \\sum_{k=1}^K \\theta_k(x-\\xi_k)^3 \\\\\n&=\\beta_0 + \\beta_1 + \\sum_{k=1}^K \\theta_k(x^3-\\xi_k^3-3x^2\\xi_k+3x\\xi_k^2)\\\\\nf'(x) &= \\beta_1 + 3  \\sum_{k=1}^K \\theta_k(x-\\xi_k)^2 \\\\\nf''(x) &= 6  \\sum_{k=1}^K \\theta_k(x-\\xi_k)\n\\end{aligned}\\]\n與先前討論的結果一樣，我們在此引入 natural cubic splines 對二階導數以及線性的限制，可得：\n\\[\\begin{aligned}\n\\sum_{k=1}^K \\theta_k\\xi_k&=0 \\\\\n\\sum_{k=1}^K \\theta_k&=0\n\\end{aligned}\\]\n至此，我們已推導出對係數的一些限制。\n我們改寫上述關係如下：\n\\[\\left\\{\\begin{matrix}\n\\theta_{K-1}+ \\theta_K = - \\sum_{k=1}^{K-2}\\theta_k \\\\\n\\xi_{K-1}\\theta_{K-1}+\\xi_{K}\\theta_{K} = - \\sum_{k=1}^{K-2} \\theta_k \\xi_k\n\\end{matrix}\\right.\\]\n經過整理，可得\n\\[\\begin{aligned}\n\\theta_{K-1} &= \\frac{\\sum_{k=1}^{K-2}\\theta_k(\\xi_k-\\xi_K)}{\\xi_K-\\xi_{K-1}}\\\\\n\\theta_{K} &= \\frac{\\sum_{k=1}^{K-2}\\theta_k(\\xi_{K-1}-\\xi_k)}{\\xi_K-\\xi_{K-1}}\n\\end{aligned}\\]\n我們將上面的關係式代入 \\(f(X)\\) ：\n\\[\\begin{aligned}\nf(x) &= \\beta_0 + \\beta_1x + \\sum_{k=1}^K \\theta_k(x-\\xi_k)^3_+ \\\\\n&= \\beta_0 + \\beta_1x + \\sum_{k=1}^{K-2} \\theta_k(x-\\xi_k)^3_+ + \\frac{\\sum_{k=1}^{K-2}\\theta_k(\\xi_k-\\xi_K)}{\\xi_K-\\xi_{K-1}}(x-\\xi_{K-1})^3_+ + \\frac{\\sum_{k=1}^{K-2}\\theta_k(\\xi_{K-1}-\\xi_k)}{\\xi_K-\\xi_{K-1}} (x-\\xi_K)^3_+\\\\\n&= \\beta_0 + \\beta_1x + \\sum_{k=1}^{K-2}\\theta_k [(\\xi_K-\\xi_k)\\frac{(x-\\xi_k)^3_+ - (x-\\xi_K)^3_+ }{\\xi_K-\\xi_k}-(\\xi_K-\\xi_k)\\frac{(x-\\xi_{K-1})^3_+ - (x-\\xi_K)^3_+ }{\\xi_K-\\xi_{K-1}}]\\\\\n&= \\beta_0 + \\beta_1x +  \\sum_{k=1}^{K-2}\\theta_k (\\xi_K-\\xi_k) + [d_k(x)-d_{K-1}(x)]\\\\\n&= \\beta_0 N_1(x) + \\beta_1 N_2(x) +  \\sum_{k=1}^{K-2}\\theta_k(\\xi_K-\\xi_k)N_{k+2}(x)\n\\end{aligned}\\]\n此即課本中所說的 natural cubic splines basis 。\n\n\n\nExample: South Africa Heart Disease\n\nlibrary(gam)\nlibrary(splines)\n\nSAheart <- read.table('/SAheart.data', fileEncoding = \"UTF-8\", sep = \",\",header = T)\n\n\nm <- gam(chd ~ ns(sbp,df=4) + ns(tobacco,df=4) + \n          ns(ldl,df=4) + famhist + ns(obesity,df=4) + \n          ns(alcohol,df=4) + ns(age,df=4), data=SAheart, family=binomial)\n\nsummary(m)\n\npar(mfrow = c(2, 3), mar = c(5, 5, 2, 0))\nplot(m, se = TRUE, residuals = TRUE, pch = 19, col = \"darkorange\")\n\n\nCall: gam(formula = chd ~ ns(sbp, df = 4) + ns(tobacco, df = 4) + ns(ldl, \n    df = 4) + famhist + ns(obesity, df = 4) + ns(alcohol, df = 4) + \n    ns(age, df = 4), family = binomial, data = SAheart)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-1.7245 -0.8265 -0.3884  0.8870  2.9589 \n\n(Dispersion Parameter for binomial family taken to be 1)\n\n    Null Deviance: 596.1084 on 461 degrees of freedom\nResidual Deviance: 457.6318 on 436 degrees of freedom\nAIC: 509.6318 \n\nNumber of Local Scoring Iterations: 6 \n\nAnova for Parametric Effects\n                     Df Sum Sq Mean Sq F value    Pr(>F)    \nns(sbp, df = 4)       4   6.31  1.5783  1.4242  0.224956    \nns(tobacco, df = 4)   4  18.09  4.5218  4.0802  0.002941 ** \nns(ldl, df = 4)       4  12.05  3.0137  2.7194  0.029290 *  \nfamhist               1  19.70 19.7029 17.7788 3.019e-05 ***\nns(obesity, df = 4)   4   3.66  0.9161  0.8266  0.508701    \nns(alcohol, df = 4)   4   1.28  0.3200  0.2887  0.885278    \nns(age, df = 4)       4  17.64  4.4100  3.9794  0.003496 ** \nResiduals           436 483.19  1.1082                      \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nWarning message in gplot.default(x = c(\"Present\", \"Absent\", \"Present\", \"Present\", :\n“The \"x\" component of \"partial for famhist\" has class \"character\"; no gplot() methods available”\n\n\n\n\n\n\n\nExample: Phoneme(音素) Recognition"
  },
  {
    "objectID": "Ch5/Sec5.4.html",
    "href": "Ch5/Sec5.4.html",
    "title": "統計學習理論",
    "section": "",
    "text": "Smoothing Splines\n在過去的 splines 方法，我們都必須去決定 knots 的位置，但在此，我們討論一個自動建立 knots 的的模型，也就是使用 knots 的最大集合，而我們透過正則化(regularization)的方式來決定模型的複雜度。因為若我們不去限制模型的複雜度，我們一定可以配適到到一條通過每個點的函數，使 \\(RSS=0\\) ，但這樣會導致模型過度配適 (overfitting) 的問題，因此，我們在此加入懲罰項，我們定義此模型的損失函數如下：\n令 \\(f(x)\\) 是二階連續可微，且定義penalized residual sum of squares為：\n\\[RSS(f,\\lambda) = \\sum_{i=1}^N\\{ y_i - f(x_i) \\}^2 + \\lambda \\int \\{f''(t) \\}^2 dt\\]\n其中， \\(\\lambda\\) 為非負的 tuning parameter 或稱 fixed smoothing parameter。 上式第一項衡量模型的配適程度，第二項針對模型曲率進行懲罰。我們可以考慮兩種極端的情形：\n\n\\(\\lambda=0\\) : \\(f\\) 可以為任意經過每個資料點的函數。\n\\(\\lambda = \\infty\\) : 因為 \\(\\lambda = \\infty\\) ，故我們一定要要求二階微分為 \\(0\\) ，故函數必須為線性，為簡單最小平方法所配適出來的直線。\n\n我們可以證明， 當我們在極小化 \\(RSS(f,\\lambda)\\) 時，會有一個最小值，是擁有不重複的 (unique) \\(x_i,\\ i=2,\\cdots,N\\) 的 knots 的 natural cubic spline。\n\n\n證明 smoothing splines (from Ex. 5.7)\nDerivation of smoothing splines (Green and Silverman, 1994). Suppose that \\(N \\geq 2\\), and that \\(g\\) is the natural cubic spline interpolant to the pairs \\(\\{x_i, z_i\\}_1^N\\) , with \\(a < x_1 < \\cdots < x_N < b\\). This is a natural spline with a knot at every \\(x_i\\); being an \\(N\\)-dimensional space of functions, we can determine the coefficients such that it interpolates the sequence \\(z_i\\) exactly. Let \\(\\tilde{g}\\) be any other differentiable function on \\([a, b]\\) that interpolates the \\(N\\) pairs.\n(a) Let \\(h(x)=\\tilde{g}(x)-g(x)\\). Use integration by parts and the fact that g is a natural cubic spline to show that\n\\[\\int_{a}^{b}g''(x)h''(x)dx = -\\sum_{j=1}^{N-1}g'''(x_j^+)\\{ h(x_{j+1})-h(x_{j}) \\}=0\\]\n依照題意，我們在此使用分部積分法來推導上式關係：\n\\[\\begin{aligned}\n\\int_{a}^{b}g''(x)h''(x)dx &= \\underbrace{g''(x)h'(x)|^b_a}_{=0,\\ g''(a)=g''(b)=0} -\\int_a^b g'''(x)h'(x)dx \\\\\n&= - \\sum_{i=1}^{N-1}\\int_{x_i}^{x_{i+1}}g'''(x)h'(x)dx \\\\\n&= - \\sum_{i=1}^{N-1} \\underbrace{ [g'''(x)h(x)]|_{x_i}^{x_{i+1}} }_{=0,\\ h(x_i)=\\tilde{g}(x_i)-g(x_i)=0,\\ \\forall i} + \\sum_{i=1}^{N-1} \\underbrace{\\int_{x_i}^{x_{i+1}}g^{(4)}(x)h(x)dx }_{=0,\\ g(\\cdot)\\ is\\ cubic }\\\\\n&=0\n\\end{aligned}\\]\n我們可以看到，我們在此使用了 natural cubic spline 對邊界配適直線的設定 (二次微分為 \\(0\\) )，得到 \\(g''(a)=g''(b)=0\\) 。而後再執行一次分部積分，可以看到根據 \\(h(\\cdot)\\) 的定義，可得 \\(h(x_i) = \\tilde{g}(x_i)-g(x_i)=0\\ ,\\forall i\\) 且 \\(g^{(4)}(x)=0\\) ，故得證。\n(b) Hence show that\n\\[\\int_{a}^{b}\\tilde{g}''(t)^2dt \\geq \\int_{a}^{b} g''(t)^2dt\\ ,\\]\nand that equality can only hold if h is identically zero in \\([a, b]\\) .\n我們可以展開不等號左方的積分如下：\n\\[\\begin{aligned}\n\\int_{a}^{b}\\tilde{g}''(t)^2dt &= \\int_{a}^{b} (g''(t)+h''(t))^2dt \\\\\n&=\\int_{a}^{b} g''(t)^2 dt + \\int_{a}^{b} h''(t)^2 dt + 2\\underbrace{\\int_{a}^{b} g''(t)h''(t)dt}_{=0,\\ by\\ (a)}\\\\\n&= \\int_{a}^{b} g''(t)^2 dt + \\int_{a}^{b} h''(t)^2 dt\n\\end{aligned}\\]\n除非在 \\([a,b],\\ h''(t)=0\\) 否則 \\(\\int_{a}^{b}\\tilde{g}''(t)^2dt \\geq \\int_{a}^{b} g''(t)^2dt\\) 必定成立，換句話說，「等號」成立於 \\(h''(t)=0,\\ \\forall t\\in [a,b]\\) 時，此條件也等同於 \\(h(t)=0,\\ \\forall t\\in [a,b]\\) 。\n(c) Consider the penalized least squares problem\n\\[\\min_f[\\sum_{i=1}^N(y-f(x_i))^2+\\lambda \\int_{a}^{b}f''(t)^2dt]\\]\nUse (b) to argue that the minimizer must be a cubic spline with knots at each of the \\(x_i\\) .\n我們令 \\(\\tilde{g}(x)\\) 是極小化目標函數的函數，且 \\(g(x)\\) 為 natural cubic spline ，故在 knots 處有 \\(g(x_i)=\\tilde{g}(x_i),\\ i=1,\\cdots,N\\) 。故如在 (b) 得到的結果：\n\\[\\int_{a}^{b}\\tilde{g}''(t)^2dt \\geq \\int_{a}^{b} g''(t)^2dt \\ \\Rightarrow \\lambda \\int_{a}^{b}\\tilde{g}''(t)^2dt \\geq \\lambda \\int_{a}^{b} g''(t)^2dt,\\ \\lambda >0\\]\n但既然 \\(\\tilde{g}(\\cdot)\\) 已經能極小化損失函數，但 \\({g}(\\cdot)\\) 的積分取值還更小，故只有 \\(\\tilde{g}(\\cdot)\\) 等價於 \\({g}(\\cdot)\\) 也就是 natural cubic spline 。\n\n\n#重制課本圖5.6\n\nbone <- read.table('/bone.data',header = T)\n\nmales = bone$gender == \"male\" \nfemales = bone$gender == \"female\"\n\nboneMaleSmooth = smooth.spline( bone[males,\"age\"], bone[males,\"spnbmd\"], df=12 )\nboneFemaleSmooth = smooth.spline( bone[females,\"age\"], bone[females,\"spnbmd\"], df=12 )\n\nplot(boneMaleSmooth, ylim=c(-0.05,0.20), col=\"blue\", type=\"l\", xlab=\"Age\", ylab=\"spnbmd\",main='smoothing spline')\npoints(bone[males,c(2,4)], col=\"blue\", pch=20)\n\nlines(boneFemaleSmooth, ylim=c(-0.05,0.20), col=\"red\")\npoints(bone[females,c(2,4)], col=\"red\", pch=20)\nlegend(\"topright\",c('Male','Female'),fill =c('blue','red' ))\n\n\n\n\n由於 smoothing spline 的解是 natural spline ，故我們可以將 \\(f(x)\\) 寫成以下形式：\n\\[f(x)=\\sum_{j=1}^N N_j(x)\\theta_j\\]\n而我們說 \\(N_j(x)\\) 是 \\(N\\) 維的 basis functions，我們的損失函數寫成矩陣的形式即為：\n\\[RSS(\\theta,\\lambda) = (\\mathbf{y}-\\mathbf{N}\\theta)^T(\\mathbf{y}-\\mathbf{N}\\theta)+\\lambda \\theta^T \\mathbf{\\Omega_N}\\theta\\]\n其中 \\(\\{\\mathbf{N} \\}_{ij}=N_j(x_i)\\) 且 \\(\\{\\mathbf{\\Omega_N} \\}_{jk} = \\int N''_j(t)N''_k(t)dt\\) 。\n可以求出： \\(\\hat{\\theta} = (\\mathbf{N}^T\\mathbf{N}+\\lambda\\mathbf{\\Omega_N})^{-1}\\mathbf{N}^T\\mathbf{y}\\)\n這是 generalized ridge regression 的形式。"
  },
  {
    "objectID": "Ch5/Sec5.5.html#nonparametric-logistic-regression",
    "href": "Ch5/Sec5.5.html#nonparametric-logistic-regression",
    "title": "統計學習理論",
    "section": "Nonparametric Logistic Regression",
    "text": "Nonparametric Logistic Regression"
  }
]