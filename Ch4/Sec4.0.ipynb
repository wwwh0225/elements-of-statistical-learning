{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Overview\n",
    "\n",
    "date: 2022-11-23\n",
    "\n",
    "author: Kevin Hong\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這一個章節，作者從簡單的線性模型開始，介紹了三個建立分類模型的方式。\n",
    "\n",
    "第一個方法先得到聯合機率分配$P(X, G)$，再透過貝氏定理得到資料點的後驗機率 $Pr(G=k|X=x)$。透過這種方式建立的模型為生成模型(generative model)，因為我們知道資料是從哪個分配抽出的。而在常態分配的假設下，我們可以得到線性的生成模型：即本章所要介紹的 Linear Discriminant Analysis (LDA) 和 Quadratic Discriminant Analysis (QDA)。\n",
    "\n",
    "[4.3 Linear Discriminant Analysis(LDA)](https://wwwh0225.github.io/elements-of-statistical-learning/Ch4/Sec4.3.html)\n",
    "\n",
    "第二個方式偏向頻率學派的思維，直接擬和後驗機率 $Pr(G=k|X=x)$，過程中透過 **最大化 (conditional) log-likelihood** 來認定模型參數。這樣的分類模型我們稱為判別模型 (discriminative model)。本章要介紹的 Logistic Regression 即屬於此類。\n",
    "\n",
    "[4.4 Logistic Regression](https://wwwh0225.github.io/elements-of-statistical-learning/Ch4/Sec4.4.html)\n",
    "\n",
    "第三個方式不依賴於統計理論，而是直接找出一個超平面 (hyperplane) 把手邊的訓練資料的類別區分開來，就例如本章提到的 Perceptron。而如果資料可以被完全正確分類，將有很多個超平面都可以達到 100% 的正確率。此時我們希望找出一個「最好」的超平面 (Optimal Seperating Hyperplane)，和各類別之間最大地保留間隔，這也是後面 SVM 的線性原型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
