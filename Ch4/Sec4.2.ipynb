{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wwwh0225/elements-of-statistical-learning/blob/main/Ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ-VJNgclD6u"
      },
      "source": [
        "# 4.2 Linear Regression of an Indicator Matrix\n",
        "若我們說， $\\mathcal{G}$ 有 $K$ 個類別，為了表示該筆資料是何種類別，我們可以建立一個 $Y$ 矩陣, $Y=(Y_1,...,Y_k)_{N \\times K}$，也就是說 $Y$ 矩陣中有 $N$ 個 $K$ 維的 row vectors。而根據線性迴歸模型，我們可得對 $Y$ 的估計為：\n",
        "\n",
        "$$ \\hat{Y}  = X(X^TX)^{-1}X^TY=X \\hat{B} $$\n",
        "\n",
        "$B_{(p+1)\\times K}$： $p$ 個 inputs 然後加上截距項。\n",
        "\n",
        "或者是從另外一個觀點，也就是我們希望極小化 $\\hat{y}$ 跟 $y$ 的距離。\n",
        "\n",
        "$$\\min_{\\bf{B}}\\sum_{i=1}^N||y_i-[(1,x_i^T) \\textbf{B} ]^T ||^2$$\n",
        "\n",
        "也就是說， $\\hat{f}(x)$ 會分類到最接近的目標群( $y_i=t_k,\\ if\\ g_i = k$ )：\n",
        "\n",
        "\n",
        "$$\\hat{G}(x)=\\mathop{\\arg\\min}\\limits_{k}||\\hat{f}(x)-t_k||^2$$\n",
        "\n",
        "#模擬Fig. 4.3\n",
        "https://esl.hohoweiya.xyz/notes/LDA/sim-4-3/index.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOyAzBOtUF9ZdyVZDyMg96I",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
