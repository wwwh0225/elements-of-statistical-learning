{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d73a94",
   "metadata": {},
   "source": [
    "# Ch14 Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c1f2c",
   "metadata": {},
   "source": [
    "## 14.5.1 Principle Components 主成分(分析)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209207f9",
   "metadata": {},
   "source": [
    "### 1. 概念\n",
    ">這章節對於主成分的刻畫，採用了 Pearson 的想法：**在一個小於 $p$ 維的低維 ($q$維) 的空間中，尋找原本資料的線性近似。**\n",
    "\n",
    "我們可以把原本資料的線性近似表達成以下的形式：\n",
    "\n",
    "\\begin{align}\n",
    "f(\\lambda) &= \\mu + V_q\\lambda \\tag{14.49}\n",
    "\\end{align}\n",
    "\n",
    "其中 $\\mu$ 是這個 $q$ 維空間的原點。$V_q$ 則為 $p \\times q$ 的矩陣，代表 $q$ 個用來展開這個低維空間的 othonormal 向量 (因此 $V_q^TV_q=I_q$)。\n",
    "\n",
    "我們的目標既然是去近似原本空間的 $p$ 維向量，尋找合理的 $f(\\lambda)$ 來最小化原本資料和低維對應之間的近似誤差 (reconstruction error) 是件很自然的事：\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mu, \\{\\lambda_i\\}, V_q} & \\sum_{i=1}^N \\|x_i-f(\\lambda_i)\\|^2  \\\\\n",
    "=& \\sum_{i=1}^N \\|x_i - \\mu - V_q\\lambda_i\\|^2 \\tag{14.50}\n",
    "\\end{align*}\n",
    "\n",
    "### 2. 推導 (14.51) (14.52) \n",
    "\n",
    "*以下內容完成了 Exercise 14.7*\n",
    "\n",
    "**(1)** 假使我們已經選好一個 $q$ 維空間準備去近似原本的資料點 (換句話說，空間原點 $\\mu$ 和 空間基底向量 $V_q$ 已知)，則令 $y_i=x_i-\\mu$，則 (14.50) 可以改寫為我們熟悉的 least square 問題：\n",
    "\n",
    "$$\n",
    "\\min_{\\{\\lambda_i\\}} \\sum_{i=1}^N \\|y_i - V_q\\lambda_i\\|^2 \\quad\\Leftrightarrow\\quad \\min_{\\lambda_i} \\|y_i - V_q\\lambda_i\\|^2 \n",
    "$$\n",
    "\n",
    "所以 $\\lambda_i = (V_q^TV_q)^{-1}V_q^Ty_i = V_q^T(x_i-\\mu)$。\n",
    "\n",
    "(第二個等式成立是因為每個 $V_q$ 中的行向量是一組 orthonormal 的向量，所以 $V_q^TV_q=I_q$)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**(2)** 定義 $H\\equiv V_qV_q^T$，由 (1) 可知 $q$ 維空間中的線性近似 $f(\\lambda) = \\mu + V_qV_q^T(x_i-\\mu) = \\mu + H(x_i-\\mu)$。我們接著找出，在給定空間基底 $V_q$ 之下，使近似差異最小的空間原點 $\\mu$。將 $f(\\lambda)$ 代入原問題，並除以個數調整(不影響最佳解)：\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mu} & \\frac{1}{N}\\sum_{i=1}^N \\|(I-H)(x_i-\\mu)\\|^2 \\\\\n",
    "=& \\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^T(I-H)^T(I-H)(x_i-\\mu)  \\\\\n",
    "=& \\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^T(I-H)(x_i-\\mu) \\\\\n",
    "=& \\left[\\frac{1}{N}\\sum_{i=1}^N x_i^T(I-H)x_i \\right] -\\mu^T(I-H)\\bar{x} - \\bar{x}^T(I-H)\\mu + \\mu^T(I-H)\\mu\n",
    "\\end{align*}\n",
    "\n",
    "其中 $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i$。\n",
    "\n",
    "因此對 $\\mu$ 而言，原問題為一個 quadratic programming 問題 ($I-H$ 為 semi-positive definite)，對 $\\mu$ 求導能得到上述問題的一階條件：\n",
    "\n",
    "$$\n",
    "(I-H)\\mu = (I-H)\\bar{x}\n",
    "$$\n",
    "\n",
    "我們當然可以選擇 $\\mu =\\bar{x}$，但注意到一點，那就是 $(I-H)$ 不是 full-rank 的矩陣，所以**有無窮多組解使前述的一階條件成立**。\n",
    "\n",
    "作為 $H=V_qV_q^T$ 的正交投影，因為 $rank(V_qV_q^T) = rank(V_q) = q$，所以 $rank(I-H) = p-q$， $(I-H)$ 的投影空間有 $p-q$ 個基底。\n",
    "\n",
    "因此一階條件的解集合 $S$ 為\n",
    "\n",
    "$$\n",
    "S = \\{\\mu| \\mu=\\bar{x}+\\sum_{j=1}^{p-q}a_ju_j\\}\n",
    "$$\n",
    "\n",
    "$u_j$ 是跟 $V_q$ 正交的單位向量 (可由 Gram-Schmidt 方法找出)，$a_j$ 為對應係數。\n",
    "\n",
    "---\n",
    "\n",
    "**(3)** 由 (1) 和 (2) 我們可以得到：\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu &= \\bar{x} \\tag{14.51} \\\\ \n",
    "\\lambda_i &= V_q^T(x_i-\\bar{x}) \\tag{14.52}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$\\mathrm{\\square}$\n",
    "\n",
    "\n",
    "### 3. 解出$V_q$\n",
    "\n",
    ">TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9523cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
