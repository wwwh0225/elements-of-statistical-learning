{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd+jnpcMJBoNnkt6wGTpoE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wwwh0225/elements-of-statistical-learning/blob/main/Ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear_Methods_for_Classification\n",
        "\n",
        "## Linear Regression of an Indicator Matrix\n",
        "若我們說， $\\mathcal{G}$ 有 $K$ 個類別，為了表示該筆資料是何種類別，我們可以建立一個 $Y$ 矩陣, $Y=(Y_1,...,Y_k)_{N \\times K}$，也就是說 $Y$ 矩陣中有 $N$ 個 $K$ 維的 row vectors。而根據線性迴歸模型，我們可得對 $Y$ 的估計為：\n",
        "\n",
        "$$ \\hat{Y}  = X(X^TX)^{-1}X^TY=X \\hat{B} $$\n",
        "\n",
        "$B_{(p+1)\\times K}$： $p$ 個 inputs 然後加上截距項。\n",
        "\n",
        "或者是從另外一個觀點，也就是我們希望極小化 $\\hat{y}$ 跟 $y$ 的距離。\n",
        "\n",
        "$$\\min_{\\bf{B}}\\sum_{i=1}^N||y_i-[(1,x_i^T) \\textbf{B} ]^T ||^2$$\n",
        "\n",
        "也就是說， $\\hat{f}(x)$ 會分類到最接近的目標群( $y_i=t_k,\\ if\\ g_i = k$ )：\n",
        "\n",
        "\n",
        "$$\\hat{G}(x)=\\mathop{\\arg\\min}\\limits_{k}||\\hat{f}(x)-t_k||^2$$\n",
        "\n",
        "#模擬Fig. 4.3\n",
        "https://esl.hohoweiya.xyz/notes/LDA/sim-4-3/index.html\n",
        "\n",
        "## Linear Discriminant Analysis(LDA)\n",
        "\n",
        "在課本的2.4節，我們知道在做分類決策時，我們是在極大化某種後驗機率(posrerior probability)，也就給定 $X=x$ 時，找一個最大可能性的分類當作分析的結果。\n",
        "也就是說：\n",
        "我們令分類所做的預測 $\\hat{G}(x)=\\mathcal{G}_k$  ( $\\mathcal{G}_k$ 為其中一種分類)；\n",
        "也就表示，當給定 $X=x$ 的條件機率之下， $\\mathcal{G}_k$ 是最有可能的出象，換成數學的語言就是： \n",
        " $P(\\mathcal{G}_k|X=x)=\\max_{l}P(G=\\mathcal{G}_l|X=x)$  \n",
        "（或者說是 $=\\max_{l}P(G=l|X=x)$ ）\n",
        "\n",
        "有這個基本觀念後，我們設定資料屬於類別 $k$ 的先驗機率為： $\\pi_k=P(G=k)$ ，而當然  $\\sum_{k=1}^K\\pi_k =1$。\n",
        "\n",
        "而透過貝氏定理，我們可以得到以下關係：\n",
        "\n",
        "$$P(G=k|X=x)=\\frac{{\\color{Red} f_k(x)}{\\color{Blue} \\pi_k}}{{\\color{DarkOrange} \\Sigma_{l=1}^k f_l(x)\\pi_l}}=\\frac{{\\color{Red} P(X|G=k)}{\\color{Blue} P(G=k)} }{{\\color{DarkOrange} \\Sigma_l P(X|G=l)P(G=l)}}$$\n",
        "\n",
        "根據上式，我們必須對 $f_k(x)$ 做一些假設，這也就是當資料是第 $k$ 類時， $X$ 的機率密度函數。我們設定各分類的機率密度服從「**多變量常態分配(Multivariate Normal Distribution)**」。\n",
        "\n",
        "$$f_k(x)=\\frac{1}{(2\\pi)^{\\frac{p}{2}}|\\Sigma_k|^\\frac{1}{2}}e^{-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)}$$\n",
        "\n",
        "要注意的是，在LDA的架構之下，所有類別的pdf均享有相同的共變數矩陣，也就是 $\\Sigma_k=\\Sigma,\\forall k$。\n",
        "\n",
        "接著，我們就可以以去比較兩兩類別之間的後驗發生機率 $P(G|X)$ ，我們在此利用對數的良好性質來分析兩者關係，假設我們現在要探討類別 $k$ 和類別 $l$ ，誰的發生機率大呢？我們用下列關係式來表達：\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\ln\\frac{P(G=k|X=x)}{P(G=l|X=x)}&=\\ln \\frac{f_k(x)\\pi_k}{f_l(x)\\pi_l}=\\ln\\frac{\\pi_k}{\\pi_l}+\\ln {f_k(x)}-\\ln{f_l(x)}\\\\\n",
        "&=\\ln\\frac{\\pi_k}{\\pi_l}-\\frac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)+\\frac{1}{2}(x-\\mu_l)^T\\Sigma^{-1}(x-\\mu_l)\\\\\n",
        "&=\\ln\\frac{\\pi_k}{\\pi_l}-\\frac{1}{2}(\\mu_k+\\mu_l)^T\\Sigma^{-1}(\\mu_k-\\mu_l)+x^T\\Sigma^{-1}(\\mu_k-\\mu_l)\n",
        "\\end{aligned}$$\n",
        "\n",
        "注意!這樣良好的線性性質是來自於我們假設兩個分類具有**相同的**共變異數矩陣。若我們對類別 $k$ 和類別 $l$ 的分界線感興趣，其分界線就是位在兩者機率密度相等之處，也就是當上式**等於0**時。\n",
        "\n",
        "透過相同的想法，我們可以建立一個**線性判別函數(linear discriminant function)** $\\delta_k(x)$ ，來決定該資料應被分配到哪一個類別，也就是 $G(x)=\\mathop{\\arg\\max}\\limits_{k}\\delta_k(x)$ 。\n",
        "線性判別函數如下所示：\n",
        "\n",
        "$$\\delta_k(x)=x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k +\\ln \\pi_k$$\n",
        "\n",
        "線性判別函數的推導來自以下的成比例關係：\n",
        "\n",
        "$$\\begin{aligned}\n",
        "P(G=k|X=x)&\\propto f_k(x)\\pi_k \\\\\n",
        "&\\propto -\\frac{1}{2}(x-\\mu_k)^T \\Sigma^{-1} (x-\\mu_k)+\\ln \\pi_k = -\\frac{1}{2}x^T\\Sigma^{-1}x+x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k +\\ln \\pi_k\\\\\n",
        "&\\propto  x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k +\\ln \\pi_k \\equiv  \\delta_k(x)\n",
        "\\end{aligned}$$\n",
        "\n",
        "正如古典的統計分析一樣，我們並無法知道母體分配的參數，故我們在進行統計學習時，則選擇使用**訓練集的資料來估計母體參數**。\n",
        "\n",
        " $$\\begin{aligned}\n",
        "&\\hat{\\pi_k}=N_k/N \\\\\n",
        " &\\hat{\\mu_k}=\\sum_{g_i=k}x_i/N_k \\\\\n",
        " &\\hat{\\Sigma}=\\sum_{k=1}^K\\sum_{g_i=k}(x_i-\\hat{\\mu_k})(x_i-\\hat{\\mu_k})^T/(N-K)\n",
        "\\end{aligned}$$\n",
        "\n",
        "其中 $N_k$ 是類別$k$在訓練集中的數量(observations)。\n",
        "\n",
        "----\n",
        "\n",
        "### 2-class LDA (from Ex. 4.2)\n",
        "\n",
        "**Suppose we have features $x \\in \\mathbb{R}^p$, a two-class response, with class sizes $N_1$, $N_2$, and the target coded as $−N/N_1$, $N/N_2$.**\n",
        "\n",
        "**(a) Show that the LDA rule classifies to class 2 if**\n",
        "$$x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1}) > \\frac{1}{2}(\\hat{\\mu_2}+\\hat{\\mu_1})^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1})-\\ln(N_2/N_1)$$ \n",
        "**and class 1 otherwise.**\n",
        "\n",
        "*Sol:*\n",
        "\n",
        "在二元的分類中，我們可以回溯到先前講到的log-odds的觀念，也就是建立此式來做比較(改寫自課本式4.9)：\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\ln\\frac{P(G=2|X=x)}{P(G=1|X=x)}\n",
        "=\\ln\\frac{\\pi_2}{\\pi_1}-\\frac{1}{2}(\\mu_2+\\mu_1)^T\\Sigma^{-1}(\\mu_2-\\mu_1)+x^T\\Sigma^{-1}(\\mu_2-\\mu_1)\n",
        "\\end{aligned}$$\n",
        "\n",
        "根據對數性質，若此式**大於0**，就表示 $P(G=2|X=x)$ 的機率相對於 $P(G=1|X=x)$ 來得高，故我們自然會將其分類到 class 2。\n",
        "而我們根據訓練集對上式做估計並且做些許代數運算即可得到決策的函數：\n",
        "$$x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1}) > \\frac{1}{2}(\\hat{\\mu_2}+\\hat{\\mu_1})\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1})-\\ln(N_2/N_1)$$\n",
        "\n",
        "**(b) Consider minimization of the least squares criterion**\n",
        "$$\\sum^N_{i=1}(y_i-\\beta_0-x_i^T\\beta)^2$$\n",
        "\n",
        "**Show that the solution $\\hat{\\beta}$ satisfies**\n",
        "\n",
        "$$[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}_B]\\beta=N(\\hat{\\mu_2}-\\hat{\\mu_1})$$\n",
        "\n",
        "where,\n",
        "$$\\hat{\\Sigma}_B=\\frac{N_1N_2}{N^2}(\\hat{\\mu_2}-\\hat{\\mu_1})(\\hat{\\mu_2}-\\hat{\\mu_1})^T$$\n",
        "\n",
        "*Sol:*\n",
        "\n",
        "首先，我們知道 $N=N_1+N_2$。\n",
        "而這個線性迴歸方程包含常數項，則根據 Normal equation 可以得到對 $\\hat{\\beta_0}$ 和 $\\hat{\\beta_1}$ 的估計：\n",
        "\n",
        "\n",
        "$$ \\begin{bmatrix}\n",
        "\\hat{\\beta_0}\\\\ \n",
        "\\hat{\\beta}\n",
        "\\end{bmatrix}=(X^TX)^{-1}X^Ty $$\n",
        "\n",
        "其中 **designed matrix** 可這樣表示：( $X$ 裡面有 $\\bf{1}$ 向量)\n",
        "\n",
        "$$X^TX=\\begin{bmatrix}\n",
        "N & \\sum_{i=1}^N x_i^T\\\\ \n",
        "\\sum_{i=1}^N x_i & \\sum_{i=1}^Nx_ix_i^T \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "其中\n",
        "\n",
        "$$\\sum_{i=1}^Nx_i =\\sum_{i=1}^{N_1}x_i+\\sum_{i=1}^{N_2}x_i = N_1\\hat{\\mu_1}+ N_2\\hat{\\mu_2}$$\n",
        "\n",
        "又知\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\hat{\\Sigma} &= \\frac{1}{N-2}\\sum_{k=1}^2\\sum_{g_i=k}(x_i-\\hat{\\mu_k})(x_i-\\hat{\\mu_k})^T \\\\\n",
        "&=\\frac{1}{N-2}[\\sum_{g_i=1}x_ix_i^T-N_1 \\mu_1 \\mu_1^T+\\sum_{g_i=2}x_ix_i^T-N_2\\mu_2 \\mu_2^T]\n",
        "\\end{aligned}$$\n",
        "\n",
        "因此\n",
        "\n",
        "$$\\sum_{i=1}^Nx_ix_i^T = (N-2)\\hat{\\Sigma}+N_1 \\mu_1 \\mu_1^T+N_2\\mu_2 \\mu_2^T$$\n",
        "\n",
        "若我們將屬於類別1的資料放在矩陣的前 $N_1$ 個，而類別2的資料 $N_2$ 個緊接在後，$X^Ty$ 可以寫成：\n",
        "\n",
        "$$ X^Ty= \\begin{bmatrix}\n",
        "1 &\\cdots  & 1 &1  & \\cdots &1 \\\\ \n",
        "x_1 &\\cdots  &x_{N_1}  &x_{N_1+1}  &\\cdots  & x_{N_1+N_2}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "-N/N_1\\\\ \n",
        "\\vdots\\\\ \n",
        "-N/N_1\\\\ \n",
        "N/N_2\\\\ \n",
        "\\vdots\\\\ \n",
        "N/N_2\n",
        "\\end{bmatrix} =\\begin{bmatrix}\n",
        "0 \\\\\n",
        "-N\\mu_1+N\\mu_2\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "整理先前計算出的這些東西代入 Normal equation，整理後可得：\n",
        "\n",
        "(註： $\\beta_0=(-\\frac{N_1}{N}\\mu_1^T-\\frac{N_2}{N}\\mu_2^T)\\beta$ )\n",
        "\n",
        "$$(N_1\\mu_1+N_2\\mu_2)(-\\frac{N_1}{N}\\mu_1^T-\\frac{N_2}{N}\\mu_2^T)\\beta+((N-2)\\hat{\\Sigma} +N_1\\mu_1\\mu_1^T+N_2\\mu_2\\mu_2^T)\\beta=N(\\mu_2-\\mu_1)$$\n",
        "\n",
        "經過一些運算，並引入題目對 $\\hat{\\Sigma_B}$ 的定義，即可得：\n",
        "\n",
        "$$[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}_B]\\beta=N(\\hat{\\mu_2}-\\hat{\\mu_1})$$\n",
        "\n",
        "\n",
        "**(c) Hence show that $\\hat{\\Sigma}_B \\beta$ is in the direction $(\\hat{\\mu_2} - \\hat{\\mu_1})$ and thus**\n",
        "\n",
        "$$\\hat{\\beta}\\propto \\hat{\\Sigma}^{-1} (\\hat{\\mu_2} - \\hat{\\mu_1})$$\n",
        "\n",
        "**Therefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.**\n",
        "\n",
        "*Sol:*\n",
        "\n",
        "若我們直接計算 $\\hat{\\Sigma}_B \\beta$ ，由於 $(\\hat{\\mu_2}-\\hat{\\mu_1})^T \\beta$  內積後是一個scalar。也就是\n",
        "\n",
        "$$\\hat{\\Sigma}_B \\beta=\\frac{N_1N_2}{N^2}(\\hat{\\mu_2}-\\hat{\\mu_1}){\\color{Red} (\\hat{\\mu_2}-\\hat{\\mu_1})^T \\beta}=\\frac{N_1N_2}{N^2}(\\hat{\\mu_2}-\\hat{\\mu_1}){\\color{Red} c}$$\n",
        "\n",
        "換句話說， $\\hat{\\Sigma}_B \\beta$ 和 $(\\hat{\\mu_2}-\\hat{\\mu_1})$ 差了 ${  \\frac{N_1N_2}{N^2}c}$ 倍。又這些常數都為正，故得知：\n",
        "$$ \\hat{\\beta} \\propto \\hat{\\Sigma}^{-1} (\\hat{\\mu_2}-\\hat{\\mu_1}) $$\n",
        "\n",
        "此即表示線性迴歸與LDA其實是有相像的運算邏輯。\n",
        "\n",
        "**(d) Show that this result holds for any (distinct) coding of the two classes.**\n",
        "\n",
        "*Sol:*\n",
        "\n",
        "在上一題中，我們並沒有特別對 $t_k$ 有特別假設，這樣的性質是來自於一開始對 $\\hat{\\Sigma}_B$ 的設計，並且與 $\\beta$ 內積後出現純數而推演出這樣的比例關係。\n",
        "\n",
        "\n",
        "**(e) Find the solution $\\hat{\\beta_0}$ (up to the same scalar multiple as in (c), and hence the predicted value $\\hat{f}(x)=\\hat{\\beta_0}+x^T\\hat{\\beta}$. Consider the following rule: classify to class 2 if $f(x) > 0$and class 1 otherwise. Show this is not the same as the LDA rule unless the classes have equal numbers of observations.**\n",
        "\n",
        "*Sol:*\n",
        "\n",
        "在先前的討論，我們知道：\n",
        "\n",
        "$$\\beta_0 = -\\frac{1}{N}(N_1\\hat{\\mu}^T_1+N_2\\hat{\\mu}^T_2)\\hat{\\beta}$$\n",
        "\n",
        "我們可改寫 $\\hat{f}(x)$ 如下：\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\hat{f}(x)&=-\\frac{1}{N}(N_1\\hat{\\mu}^T_1+N_2\\hat{\\mu}^T_2-Nx^T)    \\hat{\\beta }\\\\\n",
        "&=-\\frac{1}{N}(N_1\\hat{\\mu}^T_1+N_2\\hat{\\mu}^T_2-Nx^T) {\\color{Red} \\lambda \\hat{\\Sigma}^{-1}(\\hat{\\mu_2}-\\hat{\\mu_1})}\n",
        "\\end{aligned}$$\n",
        "\n",
        "經過展開，並重整，則可得到下式：\n",
        "\n",
        "$$x^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1}) > \\frac{1}{N}(N_2\\hat{\\mu_2}+N_1\\hat{\\mu_1})^T\\hat{\\Sigma^{-1}}(\\hat{\\mu_2}-\\hat{\\mu_1})$$\n",
        "若 $N_1=N_2$ ，則 $N_1/N=N_2/N=1/2$，而 $\\ln(N_2/N_1)=\\ln 1=0$\n",
        "\n",
        "這樣的決策準則就跟LDA一樣了！\n",
        "\n",
        "## Quadratic Discriminant Analysis (QDA)\n",
        "\n",
        "還記得先前在推導 LDA 時除了常態假設之外，還有一個變異數矩陣相等的假設，若我們放寬這個假設，這個方法就變成 QDA ，Quadratic 的部分就是來自於多變量常態分配中在 $e$ 上的 $x$ 二次式不會被對消。\n",
        "\n",
        "二次判別函數(Quadratic Discriminant function)如下：\n",
        "\n",
        "$$\\delta_k(x)=-\\frac{1}{2}\\ln|\\Sigma_k|-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)+\\ln \\pi_k$$\n",
        "\n",
        "$l$ 個類別的分界線可以這樣表示\n",
        "\n",
        "$$\\{x|\\delta_k(x)=\\delta_l(x) \\}$$\n",
        "\n",
        "QDA 有參數太多的缺點。\n",
        "\n",
        "## Regulized Discriminant Analysis (RDA)  \n",
        "![](https://esl.hohoweiya.xyz/img/04/fig4.7.png)\n",
        "\n",
        "Friedman(1989) 提出了介於LDA與QDA之間的方法，稱之為 **Regulized Discriminant Analysis (RDA)**。正則化的共變異數矩陣如下所示：\n",
        "\n",
        "$$\\hat{\\Sigma}_k(\\alpha)=\\alpha \\hat{\\Sigma}_k+(1-\\alpha)\\hat{\\Sigma},\\ \\alpha \\in[0,1]$$\n",
        "\n",
        "在此， $\\hat{\\Sigma}$ 是LDA所做的共變數矩陣，而 $\\hat{\\Sigma}_k$ 為QDA的共變數矩陣。\n",
        "\n",
        "by 課本 \n",
        "\n",
        "$$\\hat{\\Sigma}(\\gamma)=\\gamma\\hat{\\Sigma}+(1-\\gamma)\\hat{\\sigma}^2I$$\n"
      ],
      "metadata": {
        "id": "fQ-VJNgclD6u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QYufr9FiKw1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}