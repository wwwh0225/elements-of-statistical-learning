{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096c1f2c",
   "metadata": {},
   "source": [
    "---\n",
    "title: 14.5.1 Principle Components 主成分(分析)\n",
    "date: 2022-11-12\n",
    "author: Eric Huang, Kevin Hong\n",
    "subtitle: none\n",
    "abstract: none\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209207f9",
   "metadata": {},
   "source": [
    "### 1. 概念\n",
    ">這章節對於主成分的刻畫，採用了 Pearson 的想法：**在一個小於 $p$ 維的低維 ($q$維) 的空間中，尋找原本資料的線性近似。**\n",
    "\n",
    "我們可以把原本資料的線性近似表達成以下的形式：\n",
    "\n",
    "\\begin{align}\n",
    "f(\\lambda) &= \\mu + V_q\\lambda \\tag{14.49}\n",
    "\\end{align}\n",
    "\n",
    "其中 $\\mu$ 是這個 $q$ 維空間的原點。$V_q$ 則為 $p \\times q$ 的矩陣，代表 $q$ 個用來展開這個低維空間的 othonormal 向量 (因此 $V_q^TV_q=I_q$)。\n",
    "\n",
    "我們的目標既然是去近似原本空間的 $p$ 維向量，尋找合理的 $f(\\lambda)$ 來最小化原本資料和低維對應之間的近似誤差 (reconstruction error) 是件很自然的事：\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mu, \\{\\lambda_i\\}, V_q} \\quad & \\sum_{i=1}^N \\|x_i-f(\\lambda_i)\\|^2  \\\\\n",
    "=& \\sum_{i=1}^N \\|x_i - \\mu - V_q\\lambda_i\\|^2 \\tag{14.50}\n",
    "\\end{align*}\n",
    "\n",
    "### 2. 推導 (14.51) (14.52) \n",
    "\n",
    "*以下內容完成了 Exercise 14.7*\n",
    "\n",
    "(1) 假使我們已經選好一個 $q$ 維空間準備去近似原本的資料點 (換句話說，空間原點 $\\mu$ 和 空間基底向量 $V_q$ 已知)，則令 $y_i=x_i-\\mu$，則 (14.50) 可以改寫為我們熟悉的 least square 問題：\n",
    "\n",
    "$$\n",
    "\\min_{\\{\\lambda_i\\}} \\sum_{i=1}^N \\|y_i - V_q\\lambda_i\\|^2 \\quad\\Leftrightarrow\\quad \\min_{\\lambda_i} \\|y_i - V_q\\lambda_i\\|^2 \n",
    "$$\n",
    "\n",
    "所以 $\\lambda_i = (V_q^TV_q)^{-1}V_q^Ty_i = V_q^T(x_i-\\mu)$。\n",
    "\n",
    "(第二個等式成立是因為每個 $V_q$ 中的行向量是一組 orthonormal 的向量，所以 $V_q^TV_q=I_q$)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "(2) 定義 $H\\equiv V_qV_q^T$，由 (1) 可知 $q$ 維空間中的線性近似 $f(\\lambda) = \\mu + V_qV_q^T(x_i-\\mu) = \\mu + H(x_i-\\mu)$。我們接著找出在給定空間基底 $V_q$ 之下，使近似差異最小的空間原點 $\\mu$。\n",
    "\n",
    "我們將 $f(\\lambda)$ 代入原問題，並目標式由總和調整為平均(因為只在前面乘上一個常數，所以不會改變最佳解)：\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mu} & \\frac{1}{N}\\sum_{i=1}^N \\|(I-H)(x_i-\\mu)\\|^2 \\\\\n",
    "=& \\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^T(I-H)^T(I-H)(x_i-\\mu)  \\\\\n",
    "=& \\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^T(I-H)(x_i-\\mu) \\\\\n",
    "=& \\left[\\frac{1}{N}\\sum_{i=1}^N x_i^T(I-H)x_i \\right] -\\mu^T(I-H)\\bar{x} - \\bar{x}^T(I-H)\\mu + \\mu^T(I-H)\\mu\n",
    "\\end{align*}\n",
    "\n",
    "其中 $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i$。\n",
    "\n",
    "因此對 $\\mu$ 而言，原問題為一個 quadratic programming 問題 ($I-H$ 為 semi-positive definite)，對 $\\mu$ 求導能得到上述問題的一階條件：\n",
    "\n",
    "$$\n",
    "(I-H)\\mu = (I-H)\\bar{x}\n",
    "$$\n",
    "\n",
    "我們可以選擇 $\\mu =\\bar{x}$。但注意到 $(I-H)$ 不是 full-rank 的矩陣，所以**有無窮多組解使前述的一階條件成立**。\n",
    "\n",
    "$(I-H)$ 作為 $H=V_qV_q^T$ 的正交投影，$rank(V_qV_q^T) = rank(V_q) = q$，所以 $(I-H)$ 的零空間有 $q$ 個基底。\n",
    "\n",
    "因此一階條件的解集合 $S$ 為\n",
    "\n",
    "$$\n",
    "S = \\{\\mu| \\mu=\\bar{x}+\\sum_{i=1}^{q}a_iv_i\\}\n",
    "$$\n",
    "\n",
    "**換句話說，任何跟 $\\bar{x}$ 處在同一個 subspace 的向量，都可以作為展開這個空間的出發點。**\n",
    "\n",
    "---\n",
    "\n",
    "(3) 由 (1) 和 (2) 我們可以得到：\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu &= \\bar{x} \\tag{14.51} \\\\ \n",
    "\\lambda_i &= V_q^T(x_i-\\bar{x}) \\tag{14.52}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$\\mathrm{\\square}$\n",
    "\n",
    "\n",
    "### 3. 推導(14.53) (14.54)：解出$V_q$\n",
    "\n",
    "*以下內容說明為什麼要對 $X$ 做 SVD。*\n",
    "\n",
    "由前面我們知道，給定空間(即給定原點、基底)我們可以得到原空間每個點於該低維空間的最佳近似；給定基底向量，我們可以知道原點該如何設定(在空間中的任何一點皆可)。那麼我們只要決定基底向量，就可以延續前面的推論將近似點找出來。這個問題的關鍵，於是成為「如何找到一組基底向量 $V_q$ 使目標式最小」。我們可以將 (14.51) 和 (14.52) 代入目標式：\n",
    "\n",
    "不失一般性，我們假設 $\\bar{x} = 0$。則原式\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{V_q} \\ & \\sum_{i=1}^N \\|(x_i-\\bar{x})-V_qV_q^T(x_i-\\bar{x}) \\|^2 \\tag{14.53}\\\\\n",
    "=& \\sum_{i=1}^N \\|x_i-V_qV_q^Tx_i \\|^2 \\\\\n",
    "=& \\sum_{i=1}^N \\|(I-V_qV_q^T)x_i \\|^2\n",
    "\\end{align*}\n",
    "\n",
    "將標準化後的樣本 $x_i - \\bar{x}$ 堆疊成一個 $N \\times p$ 的矩陣 $X$，我們可以將上式改寫為\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{V_q} \\ & \\mathrm{tr}(X(I-V_qV_q^T)(I-V_qV_q^T))X^T) \\\\\n",
    "=& \\ \\mathrm{tr}((I-V_qV_q^T)X^TX(I-V_qV_q^T)) \\\\\n",
    "=& \\ \\mathrm{tr}(X^TX - V_qV_q^TX^TX - V_qV_q^TX^TX + V_qV_q^TX^TXV_qV_q^T) \\\\\n",
    "=& \\ \\mathrm{tr}(X^TX) - \\mathrm{tr}(V_qV_q^TX^TX)\n",
    "\\end{align*}\n",
    "\n",
    "也就是說，我們需要最大化 $\\mathrm{tr}(V_qV_q^TX^TX) = \\mathrm{tr}(V_q^TX^TXV_q) = \\sum_{i=1}^qv_i^TX^TXv_i$。\n",
    "\n",
    "對 $X$ 進行 SVD：$X = UDV^T$，所以 $X^TX=VD^2V^T$。則上式成為 \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^qv_i^TVD^2V^Tv_i=\\sum_{i=1}^qv_i^T(\\sum_{j=1}^pdi^2\\tilde{v}_i\\tilde{v}_i^T)v_i= \\sum_{i=1}^q\\sum_{j=1}^pdi^2(vi^T\\tilde{v}_i)^2\n",
    "$$\n",
    "\n",
    "其中 $d_i$ 為 $X$ 的 singular value 且 $d_1 \\geq d_2 \\geq \\cdots \\geq d_p$。為了得到加總的最大值，我們取 $v_i=\\tilde{v}_i, \\ i=1,\\cdots,q$。\n",
    "\n",
    "**結論：要找最佳的近似基底$V_q$，我們對 $X$ 做 SVD，並取前 q 個 right singular vectors.**\n",
    "\n",
    "\n",
    "$\\mathrm{\\square}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9523cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9540d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
